from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Crear una sesión Spark
spark = SparkSession.builder.appName("EscrituraParquetADL2").getOrCreate()

# Ejemplo de datos (reemplazar con tus datos reales)
data = [
    {"id": 1, "nombre": "Juan", "edad": 30},
    {"id": 2, "nombre": "Ana", "edad": 25},
    {"id": 3, "nombre": "Pedro", "edad": 40}
]

# Crear un DataFrame a partir de los datos
# Primero debemos analizar cómo vienen los datos de la API
# Suponiendo que los datos vienen en formato JSON desde la API
try:
    # Intentamos inferir el esquema automáticamente
    df = spark.createDataFrame(data)
    
    # Mostrar el esquema para analizar la estructura
    print("Esquema del DataFrame:")
    df.printSchema()
    
    # Mostrar una muestra de los datos para verificar el contenido
    print("Muestra de datos:")
    df.show(5)
    
    # Opcionalmente, podemos verificar estadísticas básicas
    print("Estadísticas básicas:")
    df.describe().show()
    
    # Verificar si hay valores nulos
    print("Conteo de valores nulos por columna:")
    for column in df.columns:
        null_count = df.filter(col(column).isNull()).count()
        print(f"Columna '{column}': {null_count} valores nulos")
        
except Exception as e:
    print(f"Error al analizar los datos: {str(e)}")
    # Si hay error, podemos intentar otras formas de crear el DataFrame
    # dependiendo del formato específico de los datos de la API

# Configurar la conexión a ADL2 usando la identidad de Microsoft ID
# No es necesario proporcionar credenciales explícitamente en un notebook de Synapse
# Spark utilizará la identidad administrada del notebook para autenticarse.

# Especificar la ruta al contenedor y la carpeta en ADL2
container_name = "<your_container_name>"  # Reemplazar con el nombre de tu contenedor
folder_path = "<your_folder_path>"  # Reemplazar con la ruta a la carpeta dentro del contenedor
adl2_path = f"abfss://{container_name}@{<your_storage_account_name>}.dfs.core.windows.net/{folder_path}"

# Escribir el DataFrame en formato parquet en ADL2
df.write.parquet(adl2_path, mode="overwrite")

# Opcional: leer el archivo parquet para verificar
df_leido = spark.read.parquet(adl2_path)
df_leido.show()

# Detener la sesión Spark
spark.stop()