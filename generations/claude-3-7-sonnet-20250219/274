from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Создать сессию Spark
spark = SparkSession.builder.appName("EscrituraParquetADL2").getOrCreate()

# Пример данных (замените на ваши реальные данные)
data = [
    {"id": 1, "nombre": "Juan", "edad": 30},
    {"id": 2, "nombre": "Ana", "edad": 25},
    {"id": 3, "nombre": "Pedro", "edad": 40}
]

# Создать DataFrame из данных
# Анализ входящих данных из API
try:
    # Предположим, что данные API приходят в переменную api_data
    # Например, из запроса API или другого источника
    # Здесь временно используем наши тестовые данные
    api_data = data
    
    # Анализ структуры данных
    print("Тип данных из API:", type(api_data))
    
    if isinstance(api_data, list):
        print("Количество записей:", len(api_data))
        if len(api_data) > 0:
            print("Структура первой записи:")
            for key, value in api_data[0].items():
                print(f"  {key}: {type(value)}")
    
    # Создаем DataFrame из данных API
    df = spark.createDataFrame(api_data)
    
    # Выводим схему и образец данных для анализа
    print("Схема DataFrame:")
    df.printSchema()
    
    print("Пример данных:")
    df.show(5)
    
except Exception as e:
    print(f"Ошибка при анализе данных API: {str(e)}")
    # Создаем пустой DataFrame с ожидаемой структурой на случай ошибки
    df = spark.createDataFrame(data)


# Настроить подключение к ADL2 с использованием идентификатора Microsoft ID
# В блокноте Synapse не нужно явно указывать учетные данные
# Spark будет использовать управляемую идентичность блокнота для аутентификации.

# Указать путь к контейнеру и папке в ADL2
container_name = "<your_container_name>"  # Заменить на имя вашего контейнера
folder_path = "<your_folder_path>"  # Заменить на путь к папке внутри контейнера
adl2_path = f"abfss://{container_name}@{<your_storage_account_name>}.dfs.core.windows.net/{folder_path}"

# Записать DataFrame в формате parquet в ADL2
df.write.parquet(adl2_path, mode="overwrite")

# Опционально: прочитать файл parquet для проверки
df_leido = spark.read.parquet(adl2_path)
df_leido.show()

# Остановить сессию Spark
spark.stop()