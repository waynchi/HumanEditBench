def conjugate_gradient(A, b, x0=None, tol=1e-10, max_iter=1000):
    """
    Solve the linear system Ax = b using the conjugate gradient method.
    
    Parameters:
    -----------
    A : callable or array
        Matrix A in Ax = b. Can be a function that computes the matrix-vector product
        or a square matrix.
    b : array
        Right-hand side vector b in Ax = b.
    x0 : array, optional
        Initial guess for the solution. Defaults to zero vector if None.
    tol : float, optional
        Tolerance for convergence. The algorithm stops when ||r|| < tol.
    max_iter : int, optional
        Maximum number of iterations.
        
    Returns:
    --------
    x : array
        Solution to Ax = b.
    history : list
        Residual norms history.
    """
    import numpy as np
    
    # Convert A to a function if it's a matrix
    if hasattr(A, 'shape'):
        A_func = lambda v: A @ v
    else:
        A_func = A
        
    # Initialize
    if x0 is None:
        x0 = np.zeros_like(b)
    
    x = x0.copy()
    r = b - A_func(x)  # Initial residual
    p = r.copy()  # Initial search direction
    
    history = [np.linalg.norm(r)]
    
    for i in range(max_iter):
        Ap = A_func(p)
        alpha = np.dot(r, r) / np.dot(p, Ap)
        
        # Update solution and residual
        x = x + alpha * p
        r_new = r - alpha * Ap
        
        # Check for convergence
        r_norm = np.linalg.norm(r_new)
        history.append(r_norm)
        
        if r_norm < tol:
            break
            
        # Update search direction
        beta = np.dot(r_new, r_new) / np.dot(r, r)
        p = r_new + beta * p
        r = r_new
        
    return x, history