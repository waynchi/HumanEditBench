from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json, schema_of_json
from pyspark.sql.types import StructType, StringType

# Create a Spark session
spark = SparkSession.builder.appName("EscrituraParquetADL2").getOrCreate()

# Example data (replace with your real data)
data = [
    {"id": 1, "nombre": "Juan", "edad": 30},
    {"id": 2, "nombre": "Ana", "edad": 25},
    {"id": 3, "nombre": "Pedro", "edad": 40}
]

# Create a DataFrame from the data
# Since data comes from an API and schema is unknown, we need to analyze it first
# Assuming data is coming as JSON strings in a list
raw_data = spark.createDataFrame([(str(item),) for item in data], ["json_data"])

# Infer schema from the JSON data
sample_json = raw_data.select("json_data").first()[0]
json_schema = schema_of_json(sample_json)
print("Inferred schema:")
print(json_schema.simpleString())

# Parse the JSON data with the inferred schema
df = raw_data.select(from_json("json_data", json_schema).alias("parsed_data")).select("parsed_data.*")

# Print dataframe schema and sample data for analysis
print("DataFrame Schema:")
df.printSchema()
print("Sample Data:")
df.show()

# Set up the connection to ADL2 using the Microsoft ID identity
# It is not necessary to provide credentials explicitly in a Synapse notebook
# Spark will use the managed identity of the notebook to authenticate.

# Specify the path to the container and folder in ADL2
container_name = "<your_container_name>"  # Replace with the name of your container
folder_path = "<your_folder_path>"  # Replace with the path to the folder within the container
adl2_path = f"abfss://{container_name}@{<your_storage_account_name>}.dfs.core.windows.net/{folder_path}"

# Write the DataFrame in parquet format to ADL2
df.write.parquet(adl2_path, mode="overwrite")

# Optional: read the parquet file to verify
df_leido = spark.read.parquet(adl2_path)
df_leido.show()

# Stop the Spark session
spark.stop()