import os
import shutil

from transformers import AutoModelForCausalLM
from peft import PeftModel

from dotenv import load_dotenv

import pickle
import torch
import json

load_dotenv()

DATA_SAVE_PATH = os.getenv("DATA_SAVE_PATH")
MODEL_PATH = os.getenv("MODEL_PATH")


def save_log_to_file(log_history, file_path, append_latest_only=False):
    """
    Saves the log history to a JSON file.
    If the file already exists, it appends to it.

    Parameters:
    - log_history: List of log entries (each entry is a dict).
    - file_path: Path to the file where logs will be saved.
    - append_latest_only: If True, only the latest log entry is appended.
    """
    # Initialize current_logs
    current_logs = []

    # If the file exists, load the current logs and append to them
    if os.path.exists(file_path):
        try:
            with open(file_path, "r") as f:
                content = f.read().strip()
                if content:
                    current_logs = json.loads(content)
                else:
                    current_logs = []
        except json.JSONDecodeError:
            print(f"Warning: {file_path} contains invalid JSON. Overwriting file.")
            current_logs = []
        except Exception as e:
            print(f"An error occurred while reading {file_path}: {e}")
            current_logs = []
    else:
        # File does not exist; current_logs remains an empty list
        pass

    # Decide whether to append the entire log history or just the latest entry
    if append_latest_only and log_history:
        # Append only the most recent epoch log
        current_logs.append(log_history[-1])
    else:
        # Append the entire log history
        current_logs.extend(log_history)

    # Save the updated log history
    try:
        with open(file_path, "w") as f:
            json.dump(current_logs, f, indent=4)
    except Exception as e:
        print(f"An error occurred while writing to {file_path}: {e}")


def clear_directory(directory, delete_directory=False):
    """
    Optimized version of the clear_directory function using os.scandir for improved performance.
    
    Optimizations:
    1. Use os.scandir() instead of os.listdir() for better performance when handling a large
       number of files and subdirectories. os.scandir() returns DirEntry objects that already contain
       file attribute information, reducing the number of system calls.
    2. Reduced overhead by iterating directly over DirEntry objects.
    
    Trade-offs:
    - os.scandir() is available in Python 3.5 and later. If compatibility with older versions is needed,
      this optimization cannot be applied.
    
    Clears all files and subdirectories within a given directory. Optionally deletes the directory itself.
    Creates the directory if it doesn't exist and delete_directory is False.

    Args:
        directory (str): The path to the directory to clear.
        delete_directory (bool): If True, delete the directory after clearing its contents. Defaults to False.

    Raises:
        OSError: If any error occurs during file or directory removal. Provides details about the failure.
    
    Example:
        clear_directory('/path/to/my/directory')
        clear_directory('/path/to/my/directory', delete_directory=True)
    """
    if not os.path.exists(directory):
        if not delete_directory:
            os.makedirs(directory)
            print(f"Directory '{directory}' created.")
        else:
            raise ValueError("Directory does not exist and delete_directory is True. Cannot proceed.")
        return

    # Use os.scandir for efficient directory traversal
    with os.scandir(directory) as entries:
        for entry in entries:
            try:
                if entry.is_dir(follow_symlinks=False):
                    shutil.rmtree(entry.path)
                    print(f"Removed directory: {entry.path}")
                else:
                    os.remove(entry.path)
                    print(f"Removed file: {entry.path}")
            except OSError as e:
                print(f"Failed to delete '{entry.path}'. Reason: {e}")
                raise  # Re-raise the exception to halt execution if a deletion fails

    if delete_directory:
        try:
            os.rmdir(directory)
            print(f"Removed directory: {directory}")
        except OSError as e:
            print(f"Failed to delete '{directory}'. Reason: {e}")
            raise


def merge_lora_model(
    model_name="pythia-31M",
    base_model_repo_name="EleutherAI/",
    model_load_path=MODEL_PATH,
    model_save_path=MODEL_PATH,
):

    my_model_path = os.path.join(model_load_path, model_name)
    param_count = model_name.lower().split("m")[0].split("-")[1]
    base_model = f"pythia-{param_count}M"

    base_model = AutoModelForCausalLM.from_pretrained(
        os.path.join(base_model_repo_name, base_model)
    )
    model = PeftModel.from_pretrained(base_model, my_model_path)
    merged_model = model.merge_and_unload()
    my_model_save_path = os.path.join(model_save_path, f"{model_name}_merged")
    merged_model.save_pretrained(my_model_save_path)


def remove_repetition(question, answer):
    if question in answer:
        return answer.replace(question, "").strip()
    return answer


def load_model(
    model_type,
    model_path=None,
    blocks_str=None,
    vanilla_model_name=None,
    host_model_name=None,
):
    """
    Loads different types of models based on the model_type parameter.

    Parameters:
    model_type (str): The type of model to load. One of 'Tuned Model', 'Vanilla Model',
                      'Transformed Model', 'Final Model', or 'Host Model'.
    model_path (str): The base path where models are stored.
    blocks_str (str): A string representing the layers or blocks used in model naming.
    vanilla_model_name (str): The name or path of the vanilla (base) model.
    host_model_name (str): The name or path of the host model.

    Returns:
    model: The loaded model object.

    Raises:
    ValueError: If an unknown model_type is provided or required parameters are missing.
    IOError: If loading the model fails.

    Example:
    model = load_model(
        model_type="Tuned Model",
        model_path="/path/to/models",
        blocks_str="1-5",
        vanilla_model_name="EleutherAI/pythia-31M"
    )
    """
    if model_type == "Tuned Model":
        model_name = vanilla_model_name.split("/")[-1]

        # save_path = os.path.join(model_path)
        # model_save_name = f"{model_name}_trained_{footer}"
        # save_path = os.path.join(save_path, model_save_name)

        tuned_model_name = f"{model_name}_trained_layers_{blocks_str}_merged"
        tuned_model = AutoModelForCausalLM.from_pretrained(
            os.path.join(model_path, f"{tuned_model_name}")
        )
        return tuned_model

    elif model_type == "Vanilla Model":
        vanilla_model = AutoModelForCausalLM.from_pretrained(vanilla_model_name)
        return vanilla_model

    elif model_type == "Transformed Model":
        name = host_model_name.split("/")[-1]
        save_path = os.path.join(model_path, f"{name}_preGRAFTED_{blocks_str}.pkl")
        with open(save_path, "rb") as f:
            transformed_model = pickle.load(f)
        return transformed_model

    elif model_type == "Final Model":
        name = host_model_name.split("/")[-1]
        model_save_name = f"{name}_GRAFTED_{blocks_str}.pkl"
        save_path = os.path.join(model_path, model_save_name)
        with open(save_path, "rb") as f:
            final_model = pickle.load(f)
        return final_model
    elif model_type == "Host Model":
        host_model = AutoModelForCausalLM.from_pretrained(host_model_name)
        return host_model

    else:
        raise ValueError(f"Unknown model type: {model_type}")


def load_batch_losses(file_path):
    """
    Loads batch loss data from a checkpoint file.

    Parameters:
    file_path (str): The path to the checkpoint file.

    Returns:
    list or None: The batch losses if available, None otherwise.

    Logs:
    An error message if loading fails.

    Example:
    batch_losses = load_batch_losses('/path/to/checkpoint.pt')
    """
    try:
        checkpoint = torch.load(file_path, map_location=torch.device("cpu"))
        batch_losses = checkpoint.get("batch_losses", None)
        if batch_losses is not None:
            logging.info(f"Batch losses loaded from {file_path}")
        else:
            logging.warning(f"No 'batch_losses' key found in checkpoint at {file_path}")
        return batch_losses
    except (FileNotFoundError, IOError, RuntimeError) as e:
        logging.error(f"Error loading checkpoint from {file_path}: {e}")
        return None
