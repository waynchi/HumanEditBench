To address the user's request, the highlighted code has been refactored into a reusable function. This encapsulates the entire workflow while maintaining the original functionality. The function handles model initialization, data loading, prompt setup, and response generation, then returns the LLM's output which is printed after the call.
python
from langchain_ollama import ChatOllama
from langchain_core.prompts.chat import ChatPromptTemplate
import json

def generate_shap_analysis():
    # Initialize the ChatOllama model
    chat_model = ChatOllama(model="llama3.2", base_url="http://localhost:11434")
    # chat_model = ChatOllama(
    #     model="llama3.1:70b",
    #     base_url="http://ada-open-webui.serveo.net/ollama",
    #     client_kwargs={
    #         "headers": {"authorization": "Bearer <MASKED>"}
    #     },
    # )

    # Load SHAP values from JSON file
    with open("Vuori_Final_Approval_2024_09_24.json", "r") as file:
        shap_values_json = json.load(file).get("shap_values")

    # Load system prompt from file
    with open("system.prompt", "r") as file:
        sys_prompt = file.read().strip()

    # Prepare the messages
    prompt = ChatPromptTemplate(
        [
            ("system", sys_prompt),
            ("human", "{user_input}"),
        ]
    )

    # Generate the response
    user_input = json.dumps(shap_values_json)  # Prepare user input
    chain = prompt | chat_model
    response = chain.invoke(user_input)
    return response.content

# Execute the function and print result
result = generate_shap_analysis()
print(result)