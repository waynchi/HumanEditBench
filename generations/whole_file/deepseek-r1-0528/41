from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Crear una sesión Spark
spark = SparkSession.builder.appName("EscrituraParquetADL2").getOrCreate()

# Ejemplo de datos (reemplazar con tus datos reales)
data = [
    {"id": 1, "nombre": "Juan", "edad": 30},
    {"id": 2, "nombre": "Ana", "edad": 25},
    {"id": 3, "nombre": "Pedro", "edad": 40}
]

# Crear un DataFrame a partir de los datos

# Para hacer la solicitud HTTP a la API
import requests

# Reemplaza con la URL real de tu API
api_url = "https://tu-api.com/endpoint"

# Opcional: Configura headers, parámetros, autenticación si es necesario
# headers = {'Authorization': 'Bearer <token>'}
# params = {'key': 'value'}

# Obtener datos de la API
response = requests.get(api_url)  # añade headers=headers, params=params si es necesario

# Verificar que la respuesta sea exitosa
if response.status_code != 200:
    raise Exception(f"Error en la solicitud: Código {response.status_code}")

# Parsear la respuesta JSON
api_data = response.json()

# Inspeccionar la estructura de los datos
print("Estructura de respuesta de la API:")
print(type(api_data))
print("\nPrimeras líneas de datos crudos:")
if isinstance(api_data, list):
    print(api_data[:min(3, len(api_data))])  # Muestra primeros 3 elementos si es lista
elif isinstance(api_data, dict):
    print(dict(list(api_data.items())[:3]))  # Muestra primeras 3 claves si es diccionario
else:
    print("Tipo de dato no reconocido")

# Convertir a DataFrame de Spark basado en la estructura
# Si la respuesta es una lista de registros:
if isinstance(api_data, list):
    df = spark.createDataFrame(api_data)
# Si es un objeto único:
elif isinstance(api_data, dict) and isinstance(api_data.get('data'), list):  
    # Caso común: diccionario que contiene lista en clave 'data'
    df = spark.createDataFrame(api_data['data'])
else:
    # Si tiene otra estructura, será necesario aplanarla posteriormente
    df = spark.createDataFrame([api_data])  # Esta línea podría necesitar ajustes
    print("\nADVERTENCIA: Se creó un DataFrame de un solo registro. Puede requerir transformación adicional")

# Análisis inicial del DataFrame
print("\nEsquema del DataFrame API:")
df.printSchema()
print("\nMuestra de datos del API (10 filas):")
df.show(10, truncate=True, vertical=False)

# Configurar la conexión a ADL2 usando la identidad de Microsoft ID
# No es necesario proporcionar credenciales explícitamente en un notebook de Synapse
# Spark utilizará la identidad administrada del notebook para autenticarse.

# Especificar la ruta al contenedor y la carpeta en ADL2
container_name = "<your_container_name>"  # Reemplazar con el nombre de tu contenedor
folder_path = "<your_folder_path>"  # Reemplazar con la ruta a la carpeta dentro del contenedor
adl2_path = f"abfss://{container_name}@{<your_storage_account_name>}.dfs.core.windows.net/{folder_path}"

# Escribir el DataFrame en formato parquet en ADL2
df.write.parquet(adl2_path, mode="overwrite")

# Opcional: leer el archivo parquet para verificar
df_leido = spark.read.parquet(adl2_path)
df_leido.show()

# Detener la sesión Spark
spark.stop()
