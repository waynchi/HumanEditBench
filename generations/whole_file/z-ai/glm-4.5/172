from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# 创建一个Spark会话
spark = SparkSession.builder.appName("EscrituraParquetADL2").getOrCreate()

# 示例数据（用你的真实数据替换）
# 首先，从API获取数据并进行分析
# 假设我们有一个函数来获取API数据
import requests

def get_api_data(url):
    """从指定的API获取数据"""
    try:
        response = requests.get(url)
        response.raise_for_status()  # 如果请求失败则抛出异常
        return response.json()  # 返回JSON格式的数据
    except requests.exceptions.RequestException as e:
        print(f"获取API数据时出错: {e}")
        return None

# 替换为你的API端点
api_url = "https://api.example.com/data"  # 替换为实际的API URL
api_data = get_api_data(api_url)

# 分析获取的数据
if api_data:
    print("成功获取数据，数据样本:")
    print(api_data[:2])  # 打印前两条记录作为样本
    
    # 检查数据结构
    if isinstance(api_data, list) and len(api_data) > 0:
        print("\n数据键:", list(api_data[0].keys()))
    else:
        print("警告: 数据格式不符合预期")
    
    # 使用API数据创建DataFrame
    df = spark.createDataFrame(api_data)
else:
    print("无法获取数据，使用示例数据作为回退")
    data = [
        {"id": 1, "nombre": "Juan", "edad": 30},
        {"id": 2, "nombre": "Ana", "edad": 25},
        {"id": 3, "nombre": "Pedro", "edad": 40}
    ]
    df = spark.createDataFrame(data)

# 配置与ADL2的连接使用Microsoft ID身份
# 在Synapse的笔记本中不需要显式提供凭据
# Spark将使用notebook的托管身份进行身份验证。

# 指定ADL2中容器和文件夹的路径
container_name = "<your_container_name>"  # 用你的容器名称替换
folder_path = "<your_folder_path>"  # 替换为容器内文件夹的路径
adl2_path = f"abfss://{container_name}@{<your_storage_account_name>}.dfs.core.windows.net/{folder_path}"

# 将DataFrame以parquet格式写入ADL2
df.write.parquet(adl2_path, mode="overwrite")

# 可选：读取parquet文件进行验证
df_leido = spark.read.parquet(adl2_path)
df_leido.show()

# 停止 Spark 会话
spark.stop()