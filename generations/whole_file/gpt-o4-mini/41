from pyspark.sql import SparkSession
from pyspark.sql.functions import col
import requests

# Crear una sesión Spark
spark = SparkSession.builder.appName("EscrituraParquetADL2").getOrCreate()

# Llamada a la API para obtener los datos (reemplazar con tu URL real)
api_url = "https://api.ejemplo.com/datos"  # Reemplaza con tu endpoint
response = requests.get(api_url)
response.raise_for_status()  # Falla si hay un error HTTP
data = response.json()

# Analizar la estructura de los datos
print(f"Número de registros obtenidos: {len(data)}")
if len(data) > 0:
    print("Ejemplo de registro:")
    from pprint import pprint
    pprint(data[0])

# Crear un DataFrame a partir de los datos obtenidos de la API
# Asumimos que 'data' es una lista de diccionarios
df = spark.createDataFrame(data)

# Configurar la conexión a ADL2 usando la identidad de Microsoft ID
# No es necesario proporcionar credenciales explícitamente en un notebook de Synapse
# Spark utilizará la identidad administrada del notebook para autenticarse.

# Especificar la ruta al contenedor y la carpeta en ADL2
container_name = "<your_container_name>"  # Reemplazar con el nombre de tu contenedor
storage_account = "<your_storage_account_name>"  # Reemplazar con tu cuenta de almacenamiento
folder_path = "<your_folder_path>"  # Reemplazar con la ruta a la carpeta dentro del contenedor
adl2_path = f"abfss://{container_name}@{storage_account}.dfs.core.windows.net/{folder_path}"

# Escribir el DataFrame en formato parquet en ADL2
df.write.parquet(adl2_path, mode="overwrite")

# Opcional: leer el archivo parquet para verificar
df_leido = spark.read.parquet(adl2_path)
df_leido.show()

# Detener la sesión Spark
spark.stop()