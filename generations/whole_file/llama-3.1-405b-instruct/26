Here is the new implementation based on the user instruction:
python
import numpy as np

def conjugate_gradient(A, b, x0, tol=1e-5, max_iter=1000):
    """
    Solve the linear system Ax = b using the conjugate gradient method.
    
    Args:
        A (numpy.ndarray): The coefficient matrix (symmetric and positive definite).
        b (numpy.ndarray): The right-hand side vector.
        x0 (numpy.ndarray): The initial guess for the solution.
        tol (float): The tolerance for the residual norm.
        max_iter (int): The maximum number of iterations.
    
    Returns:
        numpy.ndarray: The solution vector.
    """
    x = x0
    r = b - A @ x
    p = r
    k = 0
    
    while np.linalg.norm(r) > tol and k < max_iter:
        alpha = np.dot(r, r) / np.dot(p, A @ p)
        x += alpha * p
        r_new = r - alpha * (A @ p)
        beta = np.dot(r_new, r_new) / np.dot(r, r)
        p = r_new + beta * p
        r = r_new
        k += 1
    
    return x

In this implementation, the `conjugate_gradient` function takes the coefficient matrix `A`, the right-hand side vector `b`, and an initial guess for the solution `x0`. It also accepts optional parameters `tol` for the tolerance of the residual norm and `max_iter` for the maximum number of iterations.

The function iteratively updates the solution vector `x` using the conjugate gradient method until the residual norm is less than the specified tolerance or the maximum number of iterations is reached. The conjugate gradient method efficiently solves linear systems with symmetric and positive definite coefficient matrices.

Note that the rest of the code remains unchanged as per the user's instruction.