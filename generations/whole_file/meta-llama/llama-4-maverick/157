import numpy as np

def conjugate_gradient(A, b, tol=1e-10, max_iter=1000):
    """
    Solve the system Ax = b using the conjugate gradient method.

    Parameters:
    A (numpy array): A square, symmetric, positive-definite matrix.
    b (numpy array): The right-hand side vector.
    tol (float, optional): The tolerance for convergence. Defaults to 1e-10.
    max_iter (int, optional): The maximum number of iterations. Defaults to 1000.

    Returns:
    x (numpy array): The solution to the system Ax = b.
    """
    n = len(b)
    x = np.zeros(n)
    r = b - np.dot(A, x)
    p = r.copy()
    rsold = np.dot(r, r)

    for _ in range(max_iter):
        Ap = np.dot(A, p)
        alpha = rsold / np.dot(p, Ap)
        x += alpha * p
        r -= alpha * Ap
        rsnew = np.dot(r, r)
        if np.sqrt(rsnew) < tol:
            break
        p = r + (rsnew / rsold) * p
        rsold = rsnew

    return x

# Example usage:
if __name__ == "__main__":
    # Define a symmetric, positive-definite matrix A
    A = np.array([[4, 1, 0], [1, 4, 1], [0, 1, 4]])

    # Define the right-hand side vector b
    b = np.array([1, 2, 3])

    # Solve the system Ax = b using conjugate gradient
    x = conjugate_gradient(A, b)

    print("Solution:", x)