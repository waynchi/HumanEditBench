import numpy as np

def conjugate_gradients(A, b, x0, tol=1e-10, max_iter=1000):
    """
    Solves a system of linear equations Ax = b using the conjugate gradient method.

    Args:
        A (numpy array): The matrix of the system.
        b (numpy array): The right-hand side of the system.
        x0 (numpy array): The initial guess for the solution.
        tol (float, optional): The tolerance for convergence. Defaults to 1e-10.
        max_iter (int, optional): The maximum number of iterations. Defaults to 1000.

    Returns:
        numpy array: The solution to the system.
    """

    # Initialize the solution and the residual
    x = x0
    r = b - np.dot(A, x0)
    p = r.copy()
    rsold = np.dot(r, r)

    # Iterate until convergence or max iterations
    for _ in range(max_iter):
        # Calculate the coefficient alpha
        Ap = np.dot(A, p)
        alpha = rsold / np.dot(p, Ap)

        # Update the solution and the residual
        x += alpha * p
        r -= alpha * Ap

        # Calculate the new residual norm
        rsnew = np.dot(r, r)

        # Check for convergence
        if np.sqrt(rsnew) < tol:
            break

        # Update the conjugate direction
        p = r + (rsnew / rsold) * p
        rsold = rsnew

    return x

# Example usage:
A = np.array([[4, 12, -16], [12, 37, -43], [-16, -43, 98]])
b = np.array([10, 25, 35])
x0 = np.array([0, 0, 0])

x = conjugate_gradients(A, b, x0)
print("Solution:", x)