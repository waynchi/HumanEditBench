import numpy as np

def conj_grad(A, b, x0=None, max_iter=100, tol=1e-6):
    """
    Conjugate Gradient Method for solving Ax = b.

    Parameters:
    A (numpy array): Matrix A.
    b (numpy array): Vector b.
    x0 (numpy array, optional): Initial guess. Defaults to None.
    max_iter (int, optional): Maximum number of iterations. Defaults to 100.
    tol (float, optional): Tolerance. Defaults to 1e-6.

    Returns:
    x (numpy array): Solution vector x.
    """
    n = len(b)
    if x0 is None:
        x = np.zeros(n)
    else:
        x = x0

    r = b - np.dot(A, x)
    p = r.copy()
    rsold = np.dot(r, r)

    for _ in range(max_iter):
        Ap = np.dot(A, p)
        alpha = rsold / np.dot(p, Ap)
        x += alpha * p
        r -= alpha * Ap
        rsnew = np.dot(r, r)
        if np.sqrt(rsnew) < tol:
            break
        p = r + (rsnew / rsold) * p
        rsold = rsnew

    return x

# Example usage:
A = np.array([[4, 1], [1, 3]])
b = np.array([5, 6])
x0 = np.array([0, 0])

x = conj_grad(A, b, x0)
print("Solution vector x:", x)