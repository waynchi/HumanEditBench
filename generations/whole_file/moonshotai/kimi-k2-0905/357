import numpy as np

def conjugate_gradient(A, b, x0=None, tol=1e-10, max_iter=None):
    """
    Solve Ax = b using the Conjugate Gradient method.

    Parameters
    ----------
    A : array_like
        Symmetric positive-definite matrix (or function that returns A @ x).
    b : array_like
        Right-hand side vector.
    x0 : array_like, optional
        Initial guess. If None, zeros are used.
    tol : float, optional
        Convergence tolerance.
    max_iter : int, optional
        Maximum iterations. If None, defaults to len(b).

    Returns
    -------
    x : ndarray
        Approximate solution.
    info : dict
        Contains 'iterations', 'residual', 'converged'.
    """
    b = np.asarray(b, dtype=float)
    n = b.size

    if max_iter is None:
        max_iter = n

    if x0 is None:
        x = np.zeros_like(b)
    else:
        x = np.asarray(x0, dtype=float).copy()

    # Handle both matrix and linear-operator cases
    def matvec(v):
        if callable(A):
            return A(v)
        else:
            return A @ v

    r = b - matvec(x)
    p = r.copy()
    rs_old = np.dot(r, r)

    for k in range(max_iter):
        Ap = matvec(p)
        alpha = rs_old / np.dot(p, Ap)
        x += alpha * p
        r -= alpha * Ap
        rs_new = np.dot(r, r)

        if np.sqrt(rs_new) < tol:
            break

        beta = rs_new / rs_old
        p = r + beta * p
        rs_old = rs_new

    info = {
        'iterations': k + 1,
        'residual': np.sqrt(rs_new),
        'converged': np.sqrt(rs_new) < tol
    }
    return x, info