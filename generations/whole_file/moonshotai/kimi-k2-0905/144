import requests
import json
import logging

def interact_with_local_llm(prompt, base_url="http://localhost:11434"):
    """使用 Ollama API 与本地 LLM 交互。

:param prompt: LLM 的输入提示。
:param base_url: Ollama API 的基础 URL。
:return: 来自 LLM 的响应。"""
    endpoint = f"{base_url}/api/generate"
    payload = {
        "model": "llama3.2:latest",  # 替换为您的模型名称
        "prompt": prompt,
        "stream": False,  # 关闭流式返回，避免 JSON 解析错误
        "max_tokens": 2048  # 根据需要调整
    }
    headers = {
        "Content-Type": "application/json"
    }
    
    try:
        response = requests.post(endpoint, json=payload, headers=headers)
        response.raise_for_status()
        # 逐行解析 JSON 数据，仅取第一行有效 JSON
        first_line = response.text.strip().split('\n')[0]
        return json.loads(first_line).get('response', '')
    except requests.exceptions.RequestException as e:
        logging.error(f"Request error: {e}")
        return None
    except json.JSONDecodeError as e:
        logging.error(f"JSON decode error: {e}")
        return None

# 示例用法
if __name__ == "__main__":
    prompt = "Hello, how are you?"
    response = interact_with_local_llm(prompt)
    if response:
        print(f"LLM Response: {response}")