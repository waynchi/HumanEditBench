from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Create a Spark session
spark = SparkSession.builder.appName("EscrituraParquetADL2").getOrCreate()

# --- Data unknown: will come from an API, so we first analyze it -----
# TODO: call your API here and store the raw response (JSON string, list of dicts, etc.)
#       e.g.  raw_data = requests.get("https://your-api-endpoint").json()
# Then inspect its structure:
#       print(type(raw_data))
#       print(raw_data[:2])  # or print(raw_data.keys()) if it is a dict
# After you know the schema, convert it into a list of Python dicts or
# a Pandas DataFrame and finally into a Spark DataFrame named df.

# ---------------------------------------------------------------------

# Set up the connection to ADL2 using the Microsoft ID identity
# It is not necessary to provide credentials explicitly in a Synapse notebook
# Spark will use the managed identity of the notebook to authenticate.

# Specify the path to the container and folder in ADL2
container_name = "<your_container_name>"  # Replace with the name of your container
folder_path = "<your_folder_path>"  # Replace with the path to the folder within the container
adl2_path = f"abfss://{container_name}@{<your_storage_account_name>}.dfs.core.windows.net/{folder_path}"

# Write the DataFrame in parquet format to ADL2
df.write.parquet(adl2_path, mode="overwrite")

# Optional: read the parquet file to verify
df_leido = spark.read.parquet(adl2_path)
df_leido.show()

# Stop the Spark session
spark.stop()