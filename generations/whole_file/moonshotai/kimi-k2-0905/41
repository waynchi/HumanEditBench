from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Crear una sesión Spark
spark = SparkSession.builder.appName("EscrituraParquetADL2").getOrCreate()

# --- Paso 1: Descarga y análisis exploratorio de los datos que llegan de la API ---
import requests, json

# 1-A) Invocar el endpoint y capturar la respuesta
api_url = "<tu_endpoint>"          # Reemplazar con la URL real
headers = {"Authorization": "Bearer <tu_token>"}  # Si aplica
raw_response = requests.get(api_url, headers=headers)

# 1-B) Verificar que la respuesta sea JSON y mostrar un trozo sin procesar
try:
    raw_json = raw_response.json()
    print("Respuesta cruda (primeros 500 caracteres):")
    print(json.dumps(raw_json, ensure_ascii=False, indent=2)[:500])
except Exception as e:
    raise RuntimeError("La API no devolvió JSON válido") from e

# 1-C) Inspeccionar la estructura: ¿es una lista de objetos? ¿un objeto con "data"? ...
print("\nTipo del objeto raíz:", type(raw_json))
if isinstance(raw_json, dict):
    print("Claves principales:", list(raw_json.keys()))

# 1-D) Convertir a DataFrame de Spark para análisis adicional
# Si la API devuelve una lista de diccionarios:
if isinstance(raw_json, list):
    df = spark.createDataFrame(raw_json)
# Si la API devuelve un objeto y los datos están en una clave (p.ej. "data"):
elif isinstance(raw_json, dict) and "data" in raw_json:
    df = spark.createDataFrame(raw_json["data"])
else:
    raise ValueError("No se detectó una estructura lista de objetos compatible")

# 1-E) Exploración rápida
print("\nEsquema detectado:")
df.printSchema()
print("\nPrimeras filas:")
df.show(truncate=False)

# A partir de aquí el resto del flujo permanece igual
# Configurar la conexión a ADL2 usando la identidad de Microsoft ID
# No es necesario proporcionar credenciales explícitamente en un notebook de Synapse
# Spark utilizará la identidad administrada del notebook para autenticarse.

# Especificar la ruta al contenedor y la carpeta en ADL2
container_name = "<your_container_name>"  # Reemplazar con el nombre de tu contenedor
folder_path = "<your_folder_path>"  # Reemplazar con la ruta a la carpeta dentro del contenedor
adl2_path = f"abfss://{container_name}@{<your_storage_account_name>}.dfs.core.windows.net/{folder_path}"

# Escribir el DataFrame en formato parquet en ADL2
df.write.parquet(adl2_path, mode="overwrite")

# Opcional: leer el archivo parquet para verificar
df_leido = spark.read.parquet(adl2_path)
df_leido.show()

# Detener la sesión Spark
spark.stop()