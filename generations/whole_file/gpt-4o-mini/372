from pyspark.sql import SparkSession
from pyspark.sql.functions import col
import requests

# Utwórz sesję Spark
spark = SparkSession.builder.appName("EscrituraParquetADL2").getOrCreate()

# Przykład danych z API
api_url = "<your_api_endpoint>"  # Zastąp swoim adresem API
response = requests.get(api_url)

# Sprawdź odpowiedź API
if response.status_code == 200:
    data = response.json()  # Załóżmy, że odpowiedź jest w formacie JSON
else:
    data = []

# Utwórz DataFrame z danych
df = spark.createDataFrame(data)

# Skonfiguruj połączenie z ADL2 używając tożsamości Microsoft ID
# Nie jest konieczne podawanie poświadczeń jawnie w notebooku Synapse
# Spark użyje zarządzanej tożsamości notebooka do uwierzytelniania.

# Określ ścieżkę do kontenera i folderu w ADL2
container_name = "<your_container_name>"  # Zastąp nazwą swojego kontenera
folder_path = "<your_folder_path>"  # Zastąp ścieżką do folderu w kontenerze
adl2_path = f"abfss://{container_name}@{<your_storage_account_name>}.dfs.core.windows.net/{folder_path}"

# Zapisać DataFrame w formacie parquet w ADL2
df.write.parquet(adl2_path, mode="overwrite")

# Opcjonalnie: odczytać plik parquet w celu weryfikacji
df_leido = spark.read.parquet(adl2_path)
df_leido.show()

# Zatrzymać sesję Spark
spark.stop()