import os
import shutil

from transformers import AutoModelForCausalLM
from peft import PeftModel

from dotenv import load_dotenv

import pickle
import torch
import json

load_dotenv()

DATA_SAVE_PATH = os.getenv("DATA_SAVE_PATH")
MODEL_PATH = os.getenv("MODEL_PATH")


def save_log_to_file(log_history, file_path, append_latest_only=False):
    """
    Saves the log history to a JSON file.
    If the file already exists, it appends to it.

    Parameters:
    - log_history: List of log entries (each entry is a dict).
    - file_path: Path to the file where logs will be saved.
    - append_latest_only: If True, only the latest log entry is appended.
    """
    # Initialize current_logs
    current_logs = []

    # If the file exists, load the current logs and append to them
    if os.path.exists(file_path):
        try:
            with open(file_path, "r") as f:
                content = f.read().strip()
                if content:
                    current_logs = json.loads(content)
                else:
                    current_logs = []
        except json.JSONDecodeError:
            print(f"Warning: {file_path} contains invalid JSON. Overwriting file.")
            current_logs = []
        except Exception as e:
            print(f"An error occurred while reading {file_path}: {e}")
            current_logs = []
    else:
        # File does not exist; current_logs remains an empty list
        pass

    # Decide whether to append the entire log history or just the latest entry
    if append_latest_only and log_history:
        # Append only the most recent epoch log
        current_logs.append(log_history[-1])
    else:
        # Append the entire log history
        current_logs.extend(log_history)

    # Save the updated log history
    try:
        with open(file_path, "w") as f:
            json.dump(current_logs, f, indent=4)
    except Exception as e:
        print(f"An error occurred while writing to {file_path}: {e}")

def clear_directory(directory, delete_directory=False, *, use_concurrency=False, max_workers=None, verbose=True):
    """
    Clears all files and subdirectories within a given directory. Optionally deletes the directory itself.
    Creates the directory if it doesn't exist and delete_directory is False.

    Args:
        directory (str): The path to the directory to clear.
        delete_directory (bool): If True, delete the directory after clearing its contents. Defaults to False.
        use_concurrency (bool): If True, delete top-level items concurrently for potential speedups on I/O-heavy or
                                high-latency filesystems. Defaults to False to preserve original behavior.
        max_workers (int or None): Max worker threads when use_concurrency=True. Defaults to a sensible value if None.
        verbose (bool): If True, print per-item progress messages (matches original behavior). Set False to reduce I/O overhead.

    Raises:
        OSError: If any error occurs during file or directory removal. Provides details about the failure.
        ValueError: If directory does not exist and delete_directory is True.

    Example:
        clear_directory('/path/to/my/directory')
        clear_directory('/path/to/my/directory', delete_directory=True)

    Performance notes and trade-offs:
    - Fast path for full deletion: When delete_directory=True, use shutil.rmtree(directory) directly.
      Expected improvement: Fewer Python-level syscalls and no per-entry iteration.
      Trade-off: None (behavior-equivalent to clearing contents then removing the directory).
    - os.scandir(): Iteration uses os.scandir() which returns DirEntry objects, reducing extra stat calls compared to os.listdir().
      Expected improvement: Lower syscall overhead, faster traversal especially in large directories.
      Trade-off: None; same semantics.
    - Optional concurrency: When use_concurrency=True, top-level entries are deleted in parallel.
      Expected improvement: Can reduce wall-clock time for many independent top-level items, especially on networked or high-latency storage.
      Trade-offs: More CPU threads, interleaved output (if verbose), and error reporting aggregates after parallel execution.
    - Reduced overhead control: verbose flag allows disabling per-item prints to speed up large deletions.
      Expected improvement: Less stdout I/O overhead.
      Trade-off: Less visibility into progress.
    """
    if not os.path.exists(directory):
        if not delete_directory:
            os.makedirs(directory, exist_ok=True)
            if verbose:
                print(f"Directory '{directory}' created.")
        else:
            raise ValueError("Directory does not exist and delete_directory is True. Cannot proceed.")
        return

    # If we are deleting the directory itself, use the most direct and efficient approach.
    if delete_directory:
        try:
            shutil.rmtree(directory)
            if verbose:
                print(f"Removed directory: {directory}")
        except OSError as e:
            print(f"Failed to delete '{directory}'. Reason: {e}")
            raise
        return

    # Otherwise, clear the contents of the directory.
    # Use os.scandir for fewer stat calls and better performance than os.listdir + os.path.*
    def _delete_entry(entry):
        try:
            # Do not follow symlinks: directory symlinks should be unlinked, not traversed.
            if entry.is_dir(follow_symlinks=False):
                shutil.rmtree(entry.path)
                if verbose:
                    print(f"Removed directory: {entry.path}")
            else:
                # Treat files and symlinks uniformly with unlink
                os.unlink(entry.path)
                if verbose:
                    print(f"Removed file: {entry.path}")
        except OSError as e:
            print(f"Failed to delete '{entry.path}'. Reason: {e}")
            raise

    try:
        # Collect all top-level entries once (avoids iterator lifetime issues when using threads)
        with os.scandir(directory) as it:
            entries = list(it)

        if use_concurrency and entries:
            from concurrent.futures import ThreadPoolExecutor, as_completed

            # Choose a sensible default for workers if not provided. Use more threads for I/O-bound tasks.
            if max_workers is None:
                # Heuristic: 4x CPU count, capped to 32 to avoid oversubscription.
                cpu_count = os.cpu_count() or 1
                max_workers = min(32, max(4, 4 * cpu_count))

            errors = []
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_map = {executor.submit(_delete_entry, entry): entry for entry in entries}
                for fut in as_completed(future_map):
                    try:
                        fut.result()
                    except Exception as e:
                        # Collect errors to re-raise after attempting other deletions.
                        errors.append((future_map[fut].path, e))

            if errors:
                # Report the first error; others are implicitly reported via prints above.
                first_path, first_exc = errors[0]
                raise OSError(f"Failed to delete at least one item. First failure: {first_path} -> {first_exc}") from first_exc
        else:
            # Sequential deletion (preserves original behavior order)
            for entry in entries:
                _delete_entry(entry)

    except OSError:
        # Re-raise any OSError to halt execution, matching original behavior.
        raise


def merge_lora_model(
    model_name="pythia-31M",
    base_model_repo_name="EleutherAI/",
    model_load_path=MODEL_PATH,
    model_save_path=MODEL_PATH,
):

    my_model_path = os.path.join(model_load_path, model_name)
    param_count = model_name.lower().split("m")[0].split("-")[1]
    base_model = f"pythia-{param_count}M"

    base_model = AutoModelForCausalLM.from_pretrained(
        os.path.join(base_model_repo_name, base_model)
    )
    model = PeftModel.from_pretrained(base_model, my_model_path)
    merged_model = model.merge_and_unload()
    my_model_save_path = os.path.join(model_save_path, f"{model_name}_merged")
    merged_model.save_pretrained(my_model_save_path)


def remove_repetition(question, answer):
    if question in answer:
        return answer.replace(question, "").strip()
    return answer


def load_model(
    model_type,
    model_path=None,
    blocks_str=None,
    vanilla_model_name=None,
    host_model_name=None,
):
    """
    Loads different types of models based on the model_type parameter.

    Parameters:
    model_type (str): The type of model to load. One of 'Tuned Model', 'Vanilla Model',
                      'Transformed Model', 'Final Model', or 'Host Model'.
    model_path (str): The base path where models are stored.
    blocks_str (str): A string representing the layers or blocks used in model naming.
    vanilla_model_name (str): The name or path of the vanilla (base) model.
    host_model_name (str): The name or path of the host model.

    Returns:
    model: The loaded model object.

    Raises:
    ValueError: If an unknown model_type is provided or required parameters are missing.
    IOError: If loading the model fails.

    Example:
    model = load_model(
        model_type="Tuned Model",
        model_path="/path/to/models",
        blocks_str="1-5",
        vanilla_model_name="EleutherAI/pythia-31M"
    )
    """
    if model_type == "Tuned Model":
        model_name = vanilla_model_name.split("/")[-1]

        # save_path = os.path.join(model_path)
        # model_save_name = f"{model_name}_trained_{footer}"
        # save_path = os.path.join(save_path, model_save_name)

        tuned_model_name = f"{model_name}_trained_layers_{blocks_str}_merged"
        tuned_model = AutoModelForCausalLM.from_pretrained(
            os.path.join(model_path, f"{tuned_model_name}")
        )
        return tuned_model

    elif model_type == "Vanilla Model":
        vanilla_model = AutoModelForCausalLM.from_pretrained(vanilla_model_name)
        return vanilla_model

    elif model_type == "Transformed Model":
        name = host_model_name.split("/")[-1]
        save_path = os.path.join(model_path, f"{name}_preGRAFTED_{blocks_str}.pkl")
        with open(save_path, "rb") as f:
            transformed_model = pickle.load(f)
        return transformed_model

    elif model_type == "Final Model":
        name = host_model_name.split("/")[-1]
        model_save_name = f"{name}_GRAFTED_{blocks_str}.pkl"
        save_path = os.path.join(model_path, model_save_name)
        with open(save_path, "rb") as f:
            final_model = pickle.load(f)
        return final_model
    elif model_type == "Host Model":
        host_model = AutoModelForCausalLM.from_pretrained(host_model_name)
        return host_model

    else:
        raise ValueError(f"Unknown model type: {model_type}")


def load_batch_losses(file_path):
    """
    Loads batch loss data from a checkpoint file.

    Parameters:
    file_path (str): The path to the checkpoint file.

    Returns:
    list or None: The batch losses if available, None otherwise.

    Logs:
    An error message if loading fails.

    Example:
    batch_losses = load_batch_losses('/path/to/checkpoint.pt')
    """
    try:
        checkpoint = torch.load(file_path, map_location=torch.device("cpu"))
        batch_losses = checkpoint.get("batch_losses", None)
        if batch_losses is not None:
            logging.info(f"Batch losses loaded from {file_path}")
        else:
            logging.warning(f"No 'batch_losses' key found in checkpoint at {file_path}")
        return batch_losses
    except (FileNotFoundError, IOError, RuntimeError) as e:
        logging.error(f"Error loading checkpoint from {file_path}: {e}")
        return None