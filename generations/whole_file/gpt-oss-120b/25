from langchain_ollama import ChatOllama
from langchain_core.prompts.chat import ChatPromptTemplate
import json

# Initialize the ChatOllama model
chat_model = ChatOllama(model="llama3.2", base_url="http://localhost:11434")

# Load SHAP values from JSON file
with open("Vuori_Final_Approval_2024_09_24.json", "r") as file:
    shap_values_json = json.load(file).get("shap_values")

# Load system prompt from file
with open("system.prompt", "r") as file:
    sys_prompt = file.read().strip()

# ---- Fixed section ---------------------------------------------------------
# Build a chat‑prompt template correctly and run the pipeline
# 1. Use `ChatPromptTemplate.from_messages` to create the template.
# 2. Chain the template with the model using the `|` operator.
# 3. Pass a dictionary with the variable name expected by the template
#    ("user_input") when invoking the chain.
# 4. Serialize the SHAP values to a JSON string for the LLM.

template = ChatPromptTemplate.from_messages(
    [
        ("system", sys_prompt),
        ("human", "{user_input}"),
    ]
)

# Create the runnable chain (prompt → model)
chain = template | chat_model

# Invoke the chain with the SHAP values (as a JSON string)
response = chain.invoke({"user_input": json.dumps(shap_values_json)})

# Print the response from the model
print(response)
# ---------------------------------------------------------------------------
