import torch
import torch.nn as nn


class SimpleConvNet1(nn.Module):
    """
    A simple convolutional neural network without Dropout or BatchNorm.
    The size of the flattened feature map is computed automatically
    based on the input image dimensions, so the user does not need
    to provide `flattened_size` manually.
    """

    def __init__(self, input_channels: int = 3, img_height: int = 32, img_width: int = 32):
        """
        Args:
            input_channels (int): Number of channels in the input image (default 3 for RGB).
            img_height (int): Height of the input image.
            img_width (int): Width of the input image.
        """
        super().__init__()

        # First convolutional block
        self.conv1 = nn.Sequential(
            nn.Conv2d(input_channels, 32, kernel_size=3, padding=1),  # keep spatial size
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2)  # halve spatial dimensions
        )

        # Second convolutional block
        self.conv2 = nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2)
        )

        # Compute the size of the flattened tensor after the conv layers.
        # This is done once during initialization using a dummy tensor.
        with torch.no_grad():
            dummy = torch.zeros(1, input_channels, img_height, img_width)
            dummy = self.conv1(dummy)
            dummy = self.conv2(dummy)
            flattened_size = dummy.view(1, -1).size(1)

        # Fullyâ€‘connected classifier
        self.flatten = nn.Flatten()
        self.fc1 = nn.Sequential(
            nn.Linear(flattened_size, 512),
            nn.ReLU(),
            nn.Linear(512, 3)  # Assuming 3 output classes
        )

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.flatten(x)
        x = self.fc1(x)
        return x