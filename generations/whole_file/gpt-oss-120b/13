import requests
import json

def interact_with_local_llm(prompt, base_url="http://localhost:11434"):
    """
    Interact with a local LLM using the Ollama API.

    :param prompt: The input prompt for the LLM.
    :param base_url: The base URL of the Ollama API.
    :return: The response from the LLM.
    """
    endpoint = f"{base_url}/api/generate"
    payload = {
        "model": "llama3.2:latest",  # Replace with your model name
        "prompt": prompt,
        "max_tokens": 2048  # Adjust as needed
    }
    headers = {
        "Content-Type": "application/json"
    }
    
    try:
        # Use streaming to safely handle responses that contain multiple JSON objects
        with requests.post(endpoint, json=payload, headers=headers, stream=True) as response:
            response.raise_for_status()
            full_response = ""
            for line in response.iter_lines():
                if not line:
                    continue
                try:
                    # Each line should be a JSON object; extract the 'response' field
                    data = json.loads(line.decode('utf-8'))
                    full_response += data.get('response', '')
                except json.JSONDecodeError:
                    # If a line isn't valid JSON, fall back to raw text
                    full_response += line.decode('utf-8')
            return full_response
    except requests.exceptions.RequestException as e:
        # Log the exception if needed; for now just return None
        return None

# Example usage
if __name__ == "__main__":
    prompt = "Hello, how are you?"
    response = interact_with_local_llm(prompt)
    if response:
        print(f"LLM Response: {response}")