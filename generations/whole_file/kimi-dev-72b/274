from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Создать сессию Spark
spark = SparkSession.builder.appName("EscrituraParquetADL2").getOrCreate()

# Пример данных (замените на ваши реальные данные)
# Данные поступают из API, поэтому сначала их нужно проанализировать
import requests

# Пример URL API (замените на реальный URL)
api_url = "https://api.example.com/data"

# Выполнить запрос к API
response = requests.get(api_url)

# Проверить успешность запроса
if response.status_code == 200:
    data = response.json()  # Данные в формате JSON
else:
    raise Exception(f"Failed to fetch data from API: {response.status_code}")

# Создать DataFrame из данных
df = spark.createDataFrame(data)

# Настроить подключение к ADL2 с использованием идентификатора Microsoft ID
# В блокноте Synapse не нужно явно указывать учетные данные
# Spark будет использовать управляемую идентичность блокнота для аутентификации.

# Указать путь к контейнеру и папке в ADL2
container_name = "<your_container_name>"  # Заменить на имя вашего контейнера
folder_path = "<your_folder_path>"  # Заменить на путь к папке внутри контейнера
adl2_path = f"abfss://{container_name}@{<your_storage_account_name>}.dfs.core.windows.net/{folder_path}"

# Записать DataFrame в формате parquet в ADL2
df.write.parquet(adl2_path, mode="overwrite")

# Опционально: прочитать файл parquet для проверки
df_leido = spark.read.parquet(adl2_path)
df_leido.show()

# Остановить сессию Spark
spark.stop()