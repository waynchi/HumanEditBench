"""
import os
import random
import torch
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import precision_score, recall_score
from torch.nn import functional as F
from PIL import Image, ImageDraw, ImageFont
import matplotlib.pyplot as plt
import seaborn as sns
from colpali_engine.interpretability import (
    get_similarity_maps_from_embeddings,
    plot_all_similarity_maps,
)


# Path to extracted Flickr8k dataset
FLICKR8K_IMAGES_PATH = "flickr8k/Images"
FLICKR8K_CAPTIONS_PATH = "flickr8k/captions.txt"

# Function to load image-text pairs from Flickr8k
def load_flickr8k_data(images_path, captions_path, fraction=0.1):
    # Read captions file
    with open(captions_path, "r") as f:
        captions_data = f.readlines()[1:]  # Skip header

    # Parse captions
    image_text_pairs = {}
    for line in captions_data:
        image_name, caption = line.strip().split(",", 1)
        if image_name not in image_text_pairs:
            image_text_pairs[image_name] = []
        image_text_pairs[image_name].append(caption)

    # Load only a fraction of the dataset
    selected_images = random.sample(list(image_text_pairs.keys()), int(len(image_text_pairs) * fraction))
    image_text_pairs = {k: image_text_pairs[k] for k in selected_images}

    # Create pairs of images and captions
    pairs = []
    for image_name, captions in image_text_pairs.items():
        image_path = os.path.join(images_path, image_name)
        if os.path.exists(image_path):
            pairs.append((Image.open(image_path), random.choice(captions)))
    return pairs

# Function to create unrelated pairs
def create_unrelated_pairs(image_text_pairs):
    """
    Creates unrelated pairs of images and texts by randomly shuffling the texts.

    Args:
        image_text_pairs (list): A list of tuples containing (image, text) pairs.

    Returns:
        list: A list of tuples containing (image, unrelated_text).
    """
    images, texts = zip(*image_text_pairs)
    unrelated_texts = random.sample(texts, len(texts))
    return list(zip(images, unrelated_texts))


def create_visual_pairs(image_text_pairs):
    """
    Creates pairs of original and augmented images from image-text pairs.
    
    This function takes a list of image-text pairs and creates new pairs consisting
    of the original images and their augmented versions. The augmentation used
    in this implementation is a horizontal flip.

    Args:
        image_text_pairs (list): A list of tuples containing (image, text) pairs,
            where images are PIL Image objects and texts are strings.

    Returns:
        list: A list of tuples containing (original_image, augmented_image),
            where both elements are PIL Image objects.
    """
    from torchvision.transforms import ToTensor
    images, _ = zip(*image_text_pairs)
    augmented_images = [
        ToTensor()(image).flip(-1) for image in images
    ]  # Example augmentation: horizontal flip
    return list(zip(images, augmented_images))


def cosine_similarity_analysis(embeddings1, embeddings2, title):
    """
    Computes cosine similarity for matching and unrelated pairs and compares distributions.
    """
    similarities = cosine_similarity(embeddings1.cpu().numpy(), embeddings2.cpu().numpy())

    # Matching pairs: Diagonal of the similarity matrix
    matching_similarities = np.diag(similarities)

    # Unrelated pairs: Off-diagonal similarities
    unrelated_similarities = similarities[~np.eye(similarities.shape[0], dtype=bool)]

    print(f"### {title} ###")
    print(f"Mean Matching Similarity: {np.mean(matching_similarities):.4f}")
    print(f"Mean Unrelated Similarity: {np.mean(unrelated_similarities):.4f}")
    print()

    # Plot distributions
    plt.figure(figsize=(10, 6))
    sns.histplot(matching_similarities, kde=True, label="Matching Pairs", color="blue", bins=30)
    sns.histplot(unrelated_similarities, kde=True, label="Unrelated Pairs", color="red", bins=30)
    plt.title(f"{title}: Cosine Similarity Distributions")
    plt.xlabel("Cosine Similarity")
    plt.ylabel("Frequency")
    plt.legend()
    plt.show()

def retrieval_metrics(query_embeds, target_embeds, ground_truth_indices, k=5):
    """
    Computes Precision@k and Recall@k for nearest-neighbor retrieval.

    This function evaluates the effectiveness of retrieval by calculating Precision@k and Recall@k.
    Precision@k measures the accuracy of the top-k retrieved items, while Recall@k measures the ability
    to find the relevant item within the top-k retrieved items.  It assumes there's only one true
    match per query.

    Args:
        query_embeds (torch.Tensor): Embeddings of the query data.
        target_embeds (torch.Tensor): Embeddings of the target data (database).
        ground_truth_indices (list): List of indices in the target data representing the true matches for each query.
        k (int): The number of top results to consider.

    Returns:
        tuple: A tuple containing Precision@k and Recall@k.
    """
    similarities = cosine_similarity(query_embeds.cpu().numpy(), target_embeds.cpu().numpy())
    sorted_indices = np.argsort(-similarities, axis=1)[:, :k]  # Top-k indices

    # Compute metrics
    precisions = []
    recalls = []
    for i, true_idx in enumerate(ground_truth_indices):
        retrieved_indices = sorted_indices[i]
        true_positives = int(true_idx in retrieved_indices)
        precisions.append(true_positives / k)
        recalls.append(true_positives / 1)  # Only one true match per query

    mean_precision = np.mean(precisions)
    mean_recall = np.mean(recalls)

    return mean_precision, mean_recall

def plot_query_token_importance(
    pil_image,
    similarity_maps,
    query_tokens,
    alpha: float = 0.5
) -> None:
    """
    Plot a separate heatmap for each query token in the similarity_maps.
    
    Args:
        image (PIL.Image.Image): The original image (e.g., loaded via Image.open(...)).
        similarity_maps (torch.Tensor): Similarity maps between images and queries.
        query_tokens (List[str]): A list of strings for each token in the query.
        alpha: Transparency for the heatmap overlays (0=transparent, 1=opaque).
    """
    # Convert PIL to numpy
    image_np = np.array(pil_image)
    H, W = image_np.shape[:2]
    num_tokens = similarity_maps.size(0)
    if num_tokens == 1:
        axes = [plt.subplot(1, 1, 1)]
    else:
        fig, axes = plt.subplots(1, num_tokens, figsize=(5 * num_tokens, 5))
    axes = np.array(axes).reshape((1,num_tokens))
    for i in range(num_tokens):
        visual_map = similarity_maps[i].cpu().numpy()
        # visualize the mask with matrix matplotlib
        image_np_np = np.array(image_np).astype(np.float32)
        image_np_np /= 255.0
        norm = (image_np_np - np.min(image_np_np)) / (np.max(image_np_np) - np.min(image_np_np))
        axis = axes[i]
        axis.imshow(norm)
        axis.axis('off')
        # overlay the heatmap with a darker color
        try:
            font = ImageFont.load_default()
        except:
            font = ImageFont.arial() #default fallback
        size = (H // 100, W // 100)

        font_size = 12

        # Write the index in white
        axis.text(size + 5, size + 5, str(i), color='white', font=font)

    plt.tight_layout()
    plt.show()

def create_visual_pairs(image_text_pairs):
    """
    Creates pairs of original and augmented images from image-text pairs.
    
    This function takes a list of image-text pairs and creates new pairs consisting
    of the original images and their augmented versions. The augmentation used
    in this implementation is a horizontal flip.

    Args:
        image_text_pairs (list): A list of tuples containing (image, text) pairs,
            where images are PIL Image objects and texts are strings.

    Returns:
        list: A list of tuples containing (original_image, augmented_image),
            where both elements are PIL Image objects.
    """
    from torchvision.transforms import ToTensor
    images, _ = zip(*image_text_pairs)
    augmented_images = [
        ToTensor()(image).flip(-1) for image in images
    ]  # Example augmentation: horizontal flip
    return list(zip(images, augmented_images))

def create_image(patch_height, patch_width, image_path, special_color, flag):
    image = Image.open(image_path)
    # Copy patches into the image
    width = image.width
    height = image.height
    axis_x = patch_width
    axis_y = patch_height
    patches_x = width // axis_x
    patches_y = height // axis_y

    # Draw the image with colored patches
    l, r = 0, 0
    for j in range(patches_y):
        for i in range(patches_x):
            left = l
            right = r
            top = j
            bottom = j + 1
            # Draw the patch
            image.paste(image.crop((left, top, right, bottom)), (i * axis_x, j * axis_y))
            l += axis_x
            r += axis_x
    # Make center the special color background
    image.paste(Image.new('RGB', (axis_x, axis_y), special_color), (
        (patches_x - 1) * axis_x, (patches_y - 1) * axis_y,
        (patches_x - 1) * axis_x + axis_x, (patches_y - 1) * axis_y + axis_y
    ))
    return image

def evaluate_map_quality(similarity_map, patch_mask):
    """
    Evaluates a binary patch mask with related metrics: correlation, peak and overlap
    Args:
        similarity_map (np.ndarray): contains location-aware values.
        patch_mask (np.ndarray): contains patch detection

    Returns:
        dict: Consider metrics: correlation, mean_value, max_value
    """
    # Calculate metrics
    correlation = np.corrcoef(similarity_map.flatten(), patch_mask.flatten())[0, 1]
    mean_value = np.mean(similarity_map)
    max_value = np.max(similarity_map)

    return {"correlation": correlation, "mean": mean_value, "max": max_value}

def create_default_image(size, color):
    """
    Creates an image given a size and color.
    Args:
        size (tuple): this represent the size of the image.
        color (list): RGB color value for the image.

    Returns:
        RGB image: a 3D numpy array representing the sample image.
    """
    return np.full(size + (3,), color, dtype=np.uint8)

def create_image_with_patches(patch_size, image_size, color_patches, color_background,
                               special_patch_size = 2, special_color = [0,255,0]):
    """
    Creates a new image by creating a heatmap overlay
    Args:
        patch_size (int): the size of each patch
        image_size (int): the size of the image
        color_patches (tuple): a RGB (R, G, B) tuple describing the color of the main tiles
        color_background (tuple): a RGB (R, G, B) tuple describing the color of the background
        special_patch_size (int): the size of the special patch
        special_color (tuple): a RGB (R, G, B) tuple describing the color of the special patch
    """
    image = create_default_image(image_size, color_background)
    rows = image_size // patch_size
    cols = image_size // patch_size

    for i in range(rows):
        for j in range(cols):
            image[i * patch_size:(i + 1) * patch_size, j * patch_size:(j + 1) * patch_size, :] = color_patches

    image[ special_patch_size * i:( special_patch_size + 1 ) * i, \
            special_patch_size * j:( special_patch_size + 1 ) * j, :] = special_color

    return Image.fromarray(image)

def create_image_with_text(image, text, font_size):
    """Created an image and added on top of it,
    Args:
        image ([PIL.Image.Image]): the image
        text (str): the text that's attached to the image
    """
    draw = ImageDraw.Draw(image)
    try: # try to find if the specified font exists
        font = ImageFont.truetype("arial.ttf", font_size)
    except FileNotFoundError:
        font = ImageFont.default()

    img_width = image.width
    img_height = image.height
    text_width, text_height = draw.textsize(text, font=font)

    str_width, str_height = draw.textsize(text, font=font)
    positions = ((img_width - str_width) // 2, (img_height - str_height) // 2)

    # Draw the text with new position
    draw.text(positions, text, fill="white", font=font)
    return image
"""
special_color = [0,255,0]
def create_image_with_text(image, text, font_size):
    """Creates a new image with the specified text
    Args:
        image (PIL.Image.Image): The input image.
        text (str): The text to add to the image.
        font_size (int): The font size of the text.

    Returns:
        PIL.Image.Image: The image with text.
    """
    draw = ImageDraw.Draw(image)
    try:
        from PIL import ImageFont
        font = ImageFont.truetype("arial.ttf", font_size)
        width, height = draw.textsize(text, font=font)
    except IOError:
        font = ImageFont.load_default()
        width, height = draw.textsize(text, font=font)

    # Calculate the center position
    x = (image.width - width) // 2
    y = (image.height - height) // 2

    # Add the text to the image
    draw.text((x, y), text, fill="white", font=font)
    return image
def create_image_with_text(image, text, font_size):

    draw = ImageDraw.Draw(image)
    try: # try to find if the specified font exists
        font = ImageFont.truetype("arial.ttf", font_size)
    except FileNotFoundError:
        font = ImageFont.default()

    img_width = image.width
    img_height = image.height
    text_width, text_height = draw.textsize(text, font=font)

    str_width, str_height = draw.textsize(text, font=font)
    positions = ((img_width - str_width) // 2, (img_height - str_height) // 2)

    # Draw the text with new position
    draw.text(positions, text, fill="white", font=font)
    return image