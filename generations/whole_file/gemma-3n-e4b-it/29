import os
import random
import torch
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import precision_score, recall_score
from torch.nn import functional as F
from PIL import Image, ImageDraw, ImageFont
import matplotlib.pyplot as plt
import seaborn as sns
from colpali_engine.interpretability import (
    get_similarity_maps_from_embeddings,
    plot_all_similarity_maps,
)

# Path to extracted Flickr8k dataset
FLICKR8K_IMAGES_PATH = "flickr8k/Images"
FLICKR8K_CAPTIONS_PATH = "flickr8k/captions.txt"

# Function to load image-text pairs from Flickr8k
def load_flickr8k_data(images_path, captions_path, fraction=0.1):
    # Read captions file
    with open(captions_path, "r") as f:
        captions_data = f.readlines()[1:]  # Skip header

    # Parse captions
    image_text_pairs = {}
    for line in captions_data:
        image_name, caption = line.strip().split(",", 1)
        if image_name not in image_text_pairs:
            image_text_pairs[image_name] = []
        image_text_pairs[image_name].append(caption)

    # Load only a fraction of the dataset
    selected_images = random.sample(list(image_text_pairs.keys()), int(len(image_text_pairs) * fraction))
    image_text_pairs = {k: image_text_pairs[k] for k in selected_images}

    # Create pairs of images and captions
    pairs = []
    for image_name, captions in image_text_pairs.items():
        image_path = os.path.join(images_path, image_name)
        if os.path.exists(image_path):
            pairs.append((Image.open(image_path), random.choice(captions)))
    return pairs

# Function to create unrelated pairs
def create_unrelated_pairs(image_text_pairs):
    """
    Creates unrelated pairs of images and texts by randomly shuffling the texts.

    Args:
        image_text_pairs (list): A list of tuples containing (image, text) pairs.

    Returns:
        list: A list of tuples containing images and unrelated texts.
    """
    images, texts = zip(*image_text_pairs)
    unrelated_texts = random.sample(texts, len(texts))
    return list(zip(images, unrelated_texts))

def create_visual_pairs(image_text_pairs):
    """
    Creates pairs of original and augmented images from image-text pairs.
    
    This function takes a list of image-text pairs and creates new pairs consisting
    of the original images and their augmented versions. The augmentation used
    in this implementation is a horizontal flip.

    Args:
        image_text_pairs (list): A list of tuples containing (image, text) pairs,
            where images are PIL Image objects and texts are strings.

    Returns:
        list: A list of tuples containing (original_image, augmented_image) pairs,
            where both elements are PIL Image objects.
    """
    from torchvision.transforms import ToTensor
    images, _ = zip(*image_text_pairs)
    augmented_images = [
        ToTensor()(image).flip(-1) for image in images
    ]  # Example augmentation: horizontal flip
    return list(zip(images, augmented_images))

def get_embeddings(images, texts, model_id="google/siglip-base-patch16-224"):
    """
    Given lists of images and texts, returns normalized embeddings for both.
    """
    # Ensure texts is a list of strings
    if not all(isinstance(t, str) for t in texts):
        raise ValueError("All text inputs must be strings.")

    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = AutoModel.from_pretrained(model_id, ignore_mismatched_sizes=True).to(device)
    processor = AutoProcessor.from_pretrained(model_id)

    # Preprocess images and texts
    image_inputs = processor(images=images, return_tensors="pt").to(device)
    text_inputs = processor(text=texts, return_tensors="pt", padding="max_length").to(device)

    with torch.no_grad():
        image_embeds = model.get_image_features(**image_inputs)
        text_embeds = model.get_text_features(**text_inputs)

    # Normalize embeddings
    image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)
    text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)

    return image_embeds, text_embeds

def cosine_similarity_analysis(embeddings1, embeddings2, title):
    """
    Computes cosine similarity for matching and unrelated pairs and compares distributions.
    """
    similarities = cosine_similarity(embeddings1.cpu().numpy(), embeddings2.cpu().numpy())

    # Matching pairs: Diagonal of the similarity matrix
    matching_similarities = np.diag(similarities)

    # Unrelated pairs: Off-diagonal similarities
    unrelated_similarities = similarities[~np.eye(similarities.shape[0], dtype=bool)]

    print(f"### {title} ###")
    print(f"Mean Matching Similarity: {np.mean(matching_similarities):.4f}")
    print(f"Mean Unrelated Similarity: {np.mean(unrelated_similarities):.4f}")

    # Plot distributions
    plt.figure(figsize=(10, 6))
    sns.histplot(matching_similarities, kde=True, label="Matching Pairs", color="blue", bins=30)
    sns.histplot(unrelated_similarities, kde=True, label="Unrelated Pairs", color="red", bins=30)
    plt.title(f"{title}: Cosine Similarity Distributions")
    plt.xlabel("Cosine Similarity")
    plt.ylabel("Frequency")
    plt.legend()
    plt.show()

def retrieval_metrics(query_embeds, target_embeds, ground_truth_indices, k=5):
    """
    Computes Precision@k and Recall@k for nearest-neighbor retrieval.

    This function evaluates the effectiveness of retrieval by calculating Precision@k and Recall@k.
    Precision@k measures the accuracy of the top-k retrieved items, while Recall@k measures the ability
    to find the relevant item within the top-k retrieved items.  It assumes there's only one true
    match per query.

    Args:
        query_embeds (torch.Tensor): Embeddings of the query data.
        target_embeds (torch.Tensor): Embeddings of the target data (database).
        ground_truth_indices (list): List of indices in the target data representing the true matches for each query.
        k (int): The number of top-k to consider.

    Returns:
        tuple: A tuple containing Precision@k and Recall@k.
    """
    similarities = cosine_similarity(query_embeds.cpu().numpy(), target_embeds.cpu().numpy())
    sorted_indices = np.argsort(-similarities, axis=1)[:, :k]  # Top-k indices

    # Compute metrics
    precisions = []
    recalls = []
    for i, true_idx in enumerate(ground_truth_indices):
        retrieved_indices = sorted_indices[i]
        true_positives = int(true_idx in retrieved_indices)
        precisions.append(true_positives / k)
        recalls.append(true_positives / 1)

    mean_precision = np.mean(precisions)
    mean_recall = np.mean(recalls)

    return mean_precision, mean_recall

def plot_query_token_importance(
    pil_image, similarity_maps, query_tokens, alpha: float = 0.5
) -> None:
    """
    Plot a separate heatmap for each query token in the similarity_maps.
    
    Args:
        pil_image (PIL.Image.Image): The original image (e.g., loaded via Image.open(...)).
        similarity_maps (torch.Tensor): Similarity maps between images and queries. Shape = (queries, patches_x, patches_y).
        query_tokens (list): A list of strings for each token in the similarity_map.
        alpha (float): Transparency for the heatmap (0=transparent, 1=opaque).
    """
    # Convert PIL to numpy array
    image_np = np.array(pil_image)
    H, W = image_np.shape[:2]

    num_tokens = similarity_maps.shape[0]
    axes = plt.subplots(1, num_tokens, figsize=(10 * num_tokens, 5))
    for i, map in enumerate(similarity_maps):
        # Create a heatmap for the current image
        dpi = 300  # Image resolution
        cmap = 'viridis'  # Choose colormap according to your preferences
        size = (W * dpi, H * dpi)
        # Resample to a standard size and ensure data is float

        cv2 = cv2.resize(map.astype(np.float32), size, interpolation=cv2.INTER_LINEAR)
        u = np.uint8(cv2)
        # Initialize image, place image as background
        image = Image.fromarray(u)
        # Add a grayscale image containing the plot
        plot = axes[i].imshow(*image.getdata())
        axes[i].axis('off')
    plt.tight_layout()
    plt.show()

def evaluate_map(maps, expected_image, metric="epoch"):
    """
    Generates information from the given maps, expected image and one single metric
    For each patch, pixel-wise similarity to the expected
    Args:
    maps(numpy.ndarray): The images (np.array) to process
    expected_image(numpy.ndarray), the true image (np.array) to compare
    metric(str):
    """
    sample_results = np.array(maps). Shape = 384, 384
    meanValue = np.mean(X_col)
    csv_file = "generated_data.csv"
    np.savetxt(csv_file, sample_results * 100, header="value", fmt="%d")

def create_image(results, matrix_for_plot, image_name=[0], visual_object=0, colormode='rainbow'):
    results = pd.DataFrame(results)

    colorMap = plt.cm.get_cmap(colormode)
    plt.xticks(range(n=len()) , None)
    plots = plt.imshow(matrix_for_plot, cmap = colorMap[0:1])
    plt.title(str(visual_object))
    plt.axis('off')
    plt.show()


def create_numpy_style_patches(dataframe):
    """
    Convert the pandas DataFrame to numpy style patches

    Args:
        dataframe(Pandas DataFrame): A pandas DataFrame containing columns that must be
    """
    # Use a random selection of values from the data, normalizing
    shapes = dataframe.shape
    np.locations = np.random.randint(0, shapes[1], size=(shapes[0], shapes[1]))
    print([np.location] * 10)

from typing import Optional

def visualize_results(results, start_axes, axes_num):

    phase_counter = 1
    # Do the loop to extract the columns as you did before
    abs_range = results.columns
    for index, col in enumerate(abs_range):
        matrix visual_matrix = col for size in [int(length_size)]

    fig, axes = plt.subplots(1, axes_num, figsize=(12, 5))

    for i, patch in enumerate(visual_matrix):
        im = sns.heatmap(patch, cmap = 'viridis', ax = axes[i], xticks = False, yticks = False,
                        cbar = True)
        axes[i].set_title(str(patch))

    self.fig, ax = fig.subplots_subplot(1, axes_num, 0)                          # Transform
    self.self.fig.show()
def create_unrelated_pairs(image_text_pairs):
    """
    Creates unrelated pairs of images and texts by randomly shuffling the texts.

    Args:
        image_text_pairs (list): A list of tuples containing (image, text) pairs.

    Returns:
        list: A list of tuples containing images and unrelated texts.
    """
    images, texts = zip(*image_text_pairs)
    unrelated_texts = random.sample(texts, len(texts))
    return list(zip(images, unrelated_texts))

def create_visual_pairs(image_text_pairs):
    """
    Creates pairs of original and augmented images from image-text pairs.

    Args:
        image_text_pairs (list): A list of image-text tuples.

    Returns:
        list: A list of tuples containing (original_image, augmented_image) pairs,
            where both elements are PIL Image objects.
    """

    # The user fraction parameter to be None

def extract_patch_data(mask, boxsize, scaling, start, size):
    """
    Extracts patch data from an image

    Args:
        mask : An illustration with channel, size, and bounding_box
        scaling (str): The choice of operation scaling location
        start (numpy.ndarray): The coordinates of the start with size.
        size (int): The size of the output
    Returns:
        numpy.ndarray: A special image of size

    """
    newimage = (mask.toimage(scaling + start) / 256.0)
    plt.show()
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import cv2  # Import the OpenCV library
from torchvision.transforms import ToTensor
from typing import Optional

# use dataset function
