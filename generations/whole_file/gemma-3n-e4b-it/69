import numpy as np
from matplotlib import pyplot as plt
from scipy.stats import lognorm
from scipy.optimize import minimize
from scipy.integrate import quad
import pandas as pd
from tqdm import tqdm
from typing import Dict, List, Tuple
import json
import pandas as pd


class ModelRouter:
    def __init__(
        self,
        models: List[str],
        lambda_latency: float = 1.0,
        lambda_rarity: float = 1.0,
        lambda_ambiguity: float = 1.0,
    ):
        self.models = models
        self.n_models = len(models)
        self.model_to_idx = {model: idx for idx, model in enumerate(models)}
        self.lambda_latency = lambda_latency
        self.lambda_rarity = lambda_rarity
        self.lambda_ambiguity = lambda_ambiguity

        # Initialize parameters
        self.n_pairs = (self.n_models * (self.n_models - 1)) // 2
        self.theta = np.zeros(self.n_pairs)

        # Cache for battle statistics
        self.battle_counts = None
        self.battle_preferences = None

        # Cache for latency parameters
        self.latency_params = None

    def _softmax_function(self, theta: np.ndarray, temp: float = 1.0) -> np.ndarray:
        """Convert parameters to probabilities using softmax with temperature."""
        exp_theta = np.exp(theta / temp)
        return exp_theta / np.sum(exp_theta)

    def _pair_to_index(self, i: int, j: int) -> int:
        """Convert model pair indices to flat index."""
        if i > j:
            i, j = j, i
        return i * (self.n_models - 1) - (i * (i - 1)) // 2 + (j - i - 1)

    def _index_to_pair(self, idx: int) -> Tuple[int, int]:
        """Convert flat index to model pair indices."""
        i = 0
        while idx >= self.n_models - i - 1:
            idx -= self.n_models - i - 1
            i += 1
        j = i + idx + 1
        return i, j

    def fit_latency_parameters(self, completions_df: pd.DataFrame):
        """Fit log-normal parameters for each model's latency distribution."""
        self.latency_params = {}

        for model in self.models:
            model_latencies = completions_df[completions_df["model"] == model][
                "latency"
            ]
            model_latencies = model_latencies[np.isfinite(model_latencies)]

            if len(model_latencies) > 0:
                # Fit log-normal distribution
                shape, loc, scale = lognorm.fit(model_latencies, floc=0)
                # Convert to mu and sigma parameters
                mu = np.log(scale)
                sigma = shape
                self.latency_params[model] = (mu, sigma)
            else:
                print(f"Warning: No latency data for model {model}")
                self.latency_params[model] = (0, 1)  # Default parameters

        print(self.latency_params)

    def compute_battle_statistics(self, outcomes_df: pd.DataFrame):
        """Compute battle counts and preferences from outcomes data."""
        battle_counts = np.zeros((self.n_models, self.n_models))
        battle_preferences = np.zeros((self.n_models, self.n_models))

        for _, row in outcomes_df.iterrows():
            items = (
                json.loads(row["completionItems"])
                if isinstance(row["completionItems"], str)
                else row["completionItems"]
            )

            if len(items) < 2:
                continue

            # Consider only the first two models in each battle
            model1, model2 = items[0]["model"], items[1]["model"]
            if model1 not in self.model_to_idx or model2 not in self.model_to_idx:
                continue

            i, j = self.model_to_idx[model1], self.model_to_idx[model2]
            battle_counts[i, j] += 1
            battle_counts[j, i] += 1

            # Determine preference using acceptedIndex
            if row.get("acceptedIndex") == 0:
                battle_preferences[i, j] += 1
                battle_preferences[j, i] -= 1
            elif row.get("acceptedIndex") == 1:
                battle_preferences[i, j] -= 1
                battle_preferences[j, i] += 1

        self.battle_counts = battle_counts
        self.battle_preferences = battle_preferences

    def compute_latency(self):
        """Compute expected maximum latency objective using exact PDF/CDF calculation."""

        def max_latency_integrand(
            l: float, mu_i: float, sigma_i: float, mu_j: float, sigma_j: float
        ) -> float:
            """
            Compute the density function for max latency:
            f_max(l) = f(l;mu_i,sigma_i)F(l;mu_j,sigma_j) + F(l;mu_i,sigma_i)f(l;mu_j,sigma_j)
            """
            # PDF for model i
            f_i = lognorm.pdf(l, sigma_i, scale=np.exp(mu_i))
            # CDF for model j
            F_j = lognorm.cdf(l, sigma_j, scale=np.exp(mu_j))
            # PDF for model j
            f_j = lognorm.pdf(l, sigma_j, scale=np.exp(mu_j))
            # CDF for model i
            F_i = lognorm.cdf(l, sigma_i, scale=np.exp(mu_i))

            max_latency = l * (f_i * F_j + F_i * f_j)
            return max_latency

        total_latency = 0
        self.latencies = []

        for idx in range(self.n_pairs):
            i, j = self._index_to_pair(idx)
            mu_i, sigma_i = self.latency_params[self.models[i]]
            mu_j, sigma_j = self.latency_params[self.models[j]]

            # Integrate the max latency density function from 0 to infinity
            expected_max, _ = quad(
                max_latency_integrand, 0, np.inf, args=(mu_i, sigma_i, mu_j, sigma_j)
            )

            self.latencies.append(expected_max)

        self.latencies = np.array(self.latencies)

        self.normalized_latencies = (self.latencies - min(self.latencies)) / (
            max(self.latencies) - min(self.latencies)
        )

    def compute_latency_objective(self, probs: np.ndarray) -> float:

        total_normalized_latency = sum(
            [probs[idx] * self.normalized_latencies[idx] for idx in range(self.n_pairs)]
        )

        return total_normalized_latency

    def compute_rarity_objective(self, probs: np.ndarray) -> float:
        """Compute rarity objective."""
        epsilon = 1.0  # Smoothing factor
        rarity_scores = []
        total_rarity = 0
        for idx in range(self.n_pairs):
            count = self.battle_counts[self.model_to_idx[self.models[idx]], self.model_to_idx[self.models[idx]]]
            if count > 0:
                rarity_score = 1.0 / (count + epsilon)
            else:
                rarity_score = 1.0  # Assign a default value if no battles
            rarity_scores.append(rarity_score)
            total_rarity -= probs[idx] * rarity_score

        return total_rarity

    def compute_ambiguity_objective(self, probs: np.ndarray) -> float:
        """Compute ambiguity objective."""
        total_ambiguity = 0
        for idx in range(self.n_pairs):
            if self.battle_counts[self.model_to_idx[self.models[idx]], self.model_to_idx[self.models[idx]]] > 0:
                avg_preference = self.battle_preferences[self.model_to_idx[self.models[idx]], self.model_to_idx[self.models[idx]]] / self.battle_counts[self.model_to_idx[self.models[idx]], self.model_to_idx[self.models[idx]]]
                ambiguity_score = 1.0 - abs(avg_preference)
                total_ambiguity -= probs[idx] * ambiguity_score
        return total_ambiguity

    def objective_function(self, theta: np.ndarray) -> float:
        """Combined objective function for optimization."""
        probs = np.exp(theta) / np.sum(np.exp(theta))
        latency_obj = self.compute_latency_objective(probs)
        rarity_obj = self.compute_rarity_objective(probs)
        ambiguity_obj = self.compute_ambiguity_objective(probs)

        total_obj = (
            self.lambda_latency * latency_obj
            + self.lambda_rarity * rarity_obj
            + self.lambda_ambiguity * ambiguity_obj
        )

        return total_obj

    def fit(self, max_iter: int = 1000):
        """Optimize the routing parameters."""
        # Create a wrapper function that updates the progress bar
        pbar = tqdm(total=max_iter, desc="Optimizing routing parameters")
        iter_count = [0]  # Use list to allow modification in nested function

        def objective_with_progress(x):
            iter_count[0] += 1
            pbar.update(1)
            return self.objective_function(x)

        try:
            result = minimize(
                objective_with_progress,
                self.theta,
                method="L-BFGS-B",
                options={"maxiter": max_iter},
            )
            self.theta = result.x
            return result
        finally:
            pbar.close()

    def get_routing_probabilities(self, temp=1.0) -> Dict[Tuple[str, str], float]:
        """Get the optimized routing probabilities for each pair."""
        probs = self._softmax_function(theta=self.theta, temp=temp)
        routing_probs = {}
        for idx in range(self.n_pairs):
            i, j = self._index_to_pair(idx)
            routing_probs[(self.models[i], self.models[j])] = probs[idx]
        return routing_probs

    def sample_model_pair(self) -> Tuple[str, str]:
        """Sample a model pair according to the optimized distribution."""
        probs = self._softmax_function(theta=self.theta, temp=1.0)
        idx = np.random.choice(self.n_pairs, p=probs)
        i, j = self._index_to_pair(idx)
        return self.models[i], self.models[j]

def main():
    # Define the models
    models = [
        "gpt-4-turbo-preview",
        "gpt-3.5-turbo",
        "gpt-4",
        "claude-3-opus-20240229",
        "gemini-1.5-pro",
        "gemini-1.5-flash-pro",
        "claude-3-sonnet-20240311",
        "mixtral-8x7b-instruct-20240307",
        "llama-3-8b-30b-open-smart",
        "nova-21k-30b",
        "phi-2",
        "Mistral-7B-Instruct-v0.2-20240414",
        "Llama-3-8B",
        "Qwen1.5-11b",
        "Qwen2-72b",
    ]
    # Initialize the router
    router = ModelRouter(
        models=models,
        lambda_latency=1.0,
        lambda_rarity=1.0,
        lambda_ambiguity=1.0,
    )
    # Load and preprocess data (replace with your data loading)
    try:
        completions_df = pd.read_csv("completions.csv")
        outcomes_df = pd.read_csv("outcomes.csv")
    except FileNotFoundError:
        print("Error: Ensure 'completions.csv' and 'outcomes.csv' are in the correct directory.")
        return

    # Fit latency parameters
    router.fit_latency_parameters(completions_df)
    # Compute battle statistics
    router.compute_battle_statistics(outcomes_df)

    # Calculate expected latency for different temperatures
    print("Expected Latency per Temperature:")
    for temp in range(1, 11):
        print(f"Temperature {temp}: {router.compute_latency() / temp:.4f} seconds")
    print()

    # Print the probability matrix for a specific temperature
    print(f"Probability Matrix at Temperature 1: ")
    router._print_probability_matrix()

def print_probability_matrix(self, title=""):
    """Print the probability matrix for each model"""
    print(title)
    prob_matrix = self._softmax_function(theta=self.theta, temp=1.0)
    print_header = f"{'Model':<30}{'Probability':<10}"
    print(print_header)
    for i, model1 in enumerate(self.models):
        print(f"{model1:<30}", end="")
        for j, model2 in enumerate(self.models):
            print(f"{prob_matrix[i][j]:<10.3f}", end="")
        print()

if __name__ == "__main__":
    main()