from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Crear una sesión Spark
spark = SparkSession.builder.appName("EscrituraParquetADL2").getOrCreate()

# Ejemplo de datos (reemplazar con tus datos reales)
data = [
    {"id": 1, "nombre": "Juan", "edad": 30},
    {"id": 2, "nombre": "Ana", "edad": 25},
    {"id": 3, "nombre": "Pedro", "edad": 40}
]

# Crear un DataFrame a partir de los datos
# Como los datos provienen de una API y no sabemos su estructura, primero analizamos
# Asumiendo que la API devuelve un JSON que convertimos a DataFrame
# (Reemplaza 'api_url' con tu URL real y ajusta según la respuesta)
import requests
response = requests.get('https://api.example.com/data')  # Ejemplo: reemplaza con tu API
json_data = response.json()  # Asume que la API devuelve un JSON list of dicts o similar

# Analizar los datos para entender la estructura
if isinstance(json_data, list) and len(json_data) > 0:
    if isinstance(json_data[0], dict):
        # Convertir directamente a DataFrame
        df = spark.createDataFrame(json_data)
    else:
        # Manejar otros formatos si no es list of dicts (ajusta según necesidad)
        raise ValueError("Estructura de datos no soportada, analiza y ajusta el código")
else:
    raise ValueError("Datos vacíos o no list de la API")

# Para analizar: imprimir esquema y primeras filas
df.printSchema()
df.show(5)

# Configurar la conexión a ADL2 usando la identidad de Microsoft ID
# No es necesario proporcionar credenciales explícitamente en un notebook de Synapse
# Spark utilizará la identidad administrada del notebook para autenticarse.

# Especificar la ruta al contenedor y la carpeta en ADL2
container_name = "<your_container_name>"  # Reemplazar con el nombre de tu contenedor
folder_path = "<your_folder_path>"  # Reemplazar con la ruta a la carpeta dentro del contenedor
adl2_path = f"abfss://{container_name}@{<your_storage_account_name>}.dfs.core.windows.net/{folder_path}"

# Escribir el DataFrame en formato parquet en ADL2
df.write.parquet(adl2_path, mode="overwrite")

# Opcional: leer el archivo parquet para verificar
df_leido = spark.read.parquet(adl2_path)
df_leido.show()

# Detener la sesión Spark
spark.stop()