import requests

def interact_with_local_llm(prompt, base_url="http://localhost:11434"):
    """
    Interact with a local LLM using the Ollama API.

    :param prompt: The input prompt for the LLM.
    :param base_url: The base URL of the Ollama API.
    :return: The response from the LLM.
    """
    endpoint = f"{base_url}/api/generate"
    payload = {
        "model": "llama3.2:latest",  # Replace with your model name
        "prompt": prompt,
        "max_tokens": 2048  # Adjust as needed
    }
    headers = {
        "Content-Type": "application/json"
    }
    
    try:
        response = requests.post(endpoint, json=payload, headers=headers)
        response.raise_for_status()
        # Modified JSON parsing to overcome the "Extra data: line 2 column 1 (char 101)" error.
        import json
        # Sometimes the API returns multiple JSON objects concatenated in one response.
        # We split the response by lines and try to parse the first valid JSON object.
        text = response.text.strip()
        # Attempt to find the first JSON object in the response text.
        first_brace = text.find('{')
        last_brace = text.find('}', first_brace)
        if first_brace != -1 and last_brace != -1:
            # Extract substring that represents the first JSON object.
            json_str = text[first_brace:last_brace+1]
            data = json.loads(json_str)
            return data.get('response', '')
        else:
            return ''
    except requests.exceptions.RequestException as e:
        return None

# Example usage
if __name__ == "__main__":
    prompt = "Hello, how are you?"
    response = interact_with_local_llm(prompt)
    if response:
        print(f"LLM Response: {response}")