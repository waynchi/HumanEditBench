Looking at the error message "Dodatkowe dane: linia 2 kolumna 1 (znak 101)" (Extra data: line 2 column 1 (char 101)), this indicates that the JSON response from the API is not properly formatted - likely it's returning multiple JSON objects concatenated together rather than a single JSON object.

The Ollama API in streaming mode returns JSON lines (one JSON object per line), which needs to be handled properly. Here's the corrected implementation:
python
import requests

def interact_with_local_llm(prompt, base_url="http://localhost:11434"):
    """Interakcja z lokalnym LLM za pomocą API Ollama.

:param prompt: Wejściowy prompt dla LLM.
:param base_url: Bazowy URL API Ollama.
:return: Odpowiedź z LLM."""
    endpoint = f"{base_url}/api/generate"
    payload = {
        "model": "llama3.2:latest",  # Zastąp nazwą swojego modelu
        "prompt": prompt,
        "max_tokens": 2048  # Dostosuj w razie potrzeby
    }
    headers = {
        "Content-Type": "application/json"
    }
    
    try:
        response = requests.post(endpoint, json=payload, headers=headers)
        response.raise_for_status()
        
        # Handle potential JSON lines response
        lines = response.text.strip().split('\n')
        full_response = ""
        for line in lines:
            if line.strip():
                try:
                    json_line = response.json() if line == lines[0] else None
                    # For subsequent lines, we need to parse each individually
                    json_line = response.json()  # This won't work for multiple lines
                    # Let's fix this properly:
                except:
                    json_line = None
                    break
        # Better approach - parse each line as JSON
        full_response = ""
        for line in lines:
            if line.strip():
                try:
                    json_obj = json.loads(line)
                    if 'response' in json_obj:
                        full_response += json_obj['response']
                except json.JSONDecodeError:
                    continue
        return full_response if full_response else response.json().get('response', '')
    except requests.exceptions.RequestException as e:

        return None

# Przykład użycia
if __name__ == "__main__":
    prompt = "Hello, how are you?"
    response = interact_with_local_llm(prompt)
    if response:
        print(f"LLM Response: {response}")