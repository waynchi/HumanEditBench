import torch
from transformers import TrainerCallback, Trainer
import numpy as np
import re
from datasets import Dataset
import os

import json
import time  # Importando el módulo time para medir el rendimiento

from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    get_linear_schedule_with_warmup,
)
from peft import (
    get_peft_model,
    LoraConfig,
    PeftModel,
    TaskType,
)
from trl.trainer import ConstantLengthDataset
from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM
from torch.utils.data import DataLoader

from my_eval import factual_score
from my_datasets import gen_mod_dataset, format_and_load_mod_data, load_sample_data
from utils import clear_directory, merge_lora_model

from dotenv import load_dotenv


import time

load_dotenv()

DATA_SAVE_PATH = os.getenv("DATA_SAVE_PATH")
MODEL_PATH = os.getenv("MODEL_PATH")

# Simulaciones
def factual_score_dataloader(*args):
    pass

batch_size = 16

def default_data_collator(*args):
    pass

x = {}

def initialize_model_and_tokenizer(
    model_name_or_path,
    tokenizer_name_or_path=None,
    config=None,
):
    model = AutoModelForCausalLM.from_pretrained(model_name_or_path)
    if config:
        model = get_peft_model(model, config)
        # model.print_trainable_parameters()
    if tokenizer_name_or_path is None:
        tokenizer_name_or_path = model_name_or_path
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"
    return model, tokenizer


def data_collator(batch):

    inputs = {
        # .to(device),
        "input_ids": torch.stack([item["input_ids"] for item in batch]),
        # .to(device),
        "labels": torch.stack([item["labels"] for item in batch]),
    }
    return inputs


def formatting_prompts_func(example):
    output_texts = []
    for i in range(len(example["instruction"])):
        text = f"### Question: {x['question']} ### Answer: {x['answer']}"
        output_texts.append(text)
    return output_texts


def train_model(
    dataset, model, tokenizer, training_args, callbacks=None, verbose=False
):
    # Dividir conjunto de datos
    train_test_split = dataset.train_test_split(test_size=0.2)

    # Crear instancias de ConstantLengthDataset
    train_dataset = ConstantLengthDataset(
        tokenizer,
        train_test_split["train"],
        formatting_func=lambda x: f"### Question: {x['question']} ### Answer: {x['answer']}",
        seq_length=18,
        num_of_sequences=20,
    )

    eval_dataset = ConstantLengthDataset(
        tokenizer,
        train_test_split["test"],
        formatting_func=lambda x: f"### Question: {x['question']} ### Answer: {x['answer']}",
        seq_length=18,
        num_of_sequences=20,
    )

    # optimizador = AdamW(model.parameters(), lr=training_args.learning_rate)

    # num_epochs = training_args.num_train_epochs
    # num_warmup_steps = int(0.1 * len(train_dataset))  # 10% de los pasos de entrenamiento
    # total_training_steps = len(train_dataset) * num_epochs
    # # Configurar el planificador
    # programador = get_linear_schedule_with_warmup(
    # optimizador,
    # num_warmup_steps=num_warmup_steps,
    # num_training_steps=total_training_steps,
    # )

    collator = DataCollatorForCompletionOnlyLM(
        " ### Answer: ",
        tokenizer=tokenizer,
    )  # Debe coincidir con formatting_func

    trainer = SFTTrainer(
        model=model,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        # optimizadores=None,#(optimizador, planificador),
        data_collator=data_collator,  # Usa el collator que definiste
        # función_de_formato=función_de_formato_de_prompts,
        packing=False,
        callbacks=callbacks,
        args=SFTConfig(**training_args.to_dict()),
    )

    if verbose:
        print("Training init done. Starting training...")
        start_time = time.time()

    trainer.train()

    if verbose:
        print(f"Training completed in {time.time() - start_time:.2f} seconds.")
        print("Starting evaluation...")
        start_time = time.time()

    trainer.evaluate()

    if verbose:
        print(f"Evaluation completed in {time.time() - start_time:.2f} seconds.")

    return trainer


def setup_training_args(
    save_path,
    model_name,
    learning_rate,
    num_epochs,
    total_train_examples,
    batch_size=1024,
    footer="",
):
    if len(footer) == 0:
        checkpoint_dir = os.path.join(save_path, model_name + "_checkpoints")
        logging_dir = os.path.join(save_path, model_name + "_logs")
    else:
        checkpoint_dir = os.path.join(save_path, model_name + f"_checkpoints_{footer}")
        logging_dir = os.path.join(save_path, model_name + f"_logs_{footer}")

    clear_directory(checkpoint_dir)

    # Calcular los pasos por época basados en el número total de ejemplos de entrenamiento y el tamaño del lote
    steps_per_epoch = total_train_examples // batch_size

    # Calcular eval_steps, save_steps y logging_steps basados en los pasos por época
    eval_steps = max(
        1, steps_per_epoch // 10
    )  # Evaluar 10 veces por época, al menos una vez por época
    save_steps = steps_per_epoch  # Guardar al final de cada época
    logging_steps = max(
        1, steps_per_epoch // 20
    )  # Registrar 20 veces por época, al menos una vez por época

    return TrainingArguments(
        output_dir=checkpoint_dir,
        learning_rate=learning_rate,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        num_train_epochs=num_epochs,
        eval_steps=eval_steps,
        save_steps=save_steps,
        logging_steps=logging_steps,
        save_total_limit=2,  # Mantener solo los 2 mejores puntos de control
        weight_decay=0.01,
        evaluation_strategy="steps",  # Cambiado a pasos para hacer uso de eval_steps
        logging_strategy="steps",
        save_strategy="steps",  # Cambiado a pasos para hacer uso de save_steps
        logging_dir=logging_dir,  # Descomenta esto si defines un directorio de registro
        report_to="none",  # Informar a tensorboard para monitoreo visual
        load_best_model_at_end=True,
        dataloader_pin_memory=False,
    )



class FactualAccuracyCallbackBETTER(TrainerCallback):
    """Un callback para evaluar y registrar la precisión factual del modelo durante el entrenamiento."""

    def __init__(
        self, model, tokenizer, dataset, verbose=False, output_format=False
    ):
        super().__init__()
        self.model = model
        self.tokenizer = tokenizer
        self.n_samp = len(dataset)
        self.verbose = verbose
        self.output_format = output_format
        self.dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=True,
            collate_fn=default_data_collator  # Función de colación para rellenar y formatear
        )

    def on_log(self, args, state, control, model=None, **kwargs):
        """Llamado después de registrar los últimos registros."""
        if model is not None:
            self.model = model
        elif self.model is None:
            print("Model is not available.")
            return

        if (
            state.is_local_process_zero
        ):  # Solo registrar desde el proceso principal cada 100 pasos
            start_time = time.time()
            try:
                if self.output_format:
                    fact_results, format_hard_results, format_soft_results = (
                        factual_score_dataloader(
                            model=self.model,
                            tokenizer=self.tokenizer,
                            dataloader=self.dataloader,
                            output_format=self.output_format,
                        )
                    )
                    # Calcular y registrar el resultado formateado
                    format_hard_avg = sum(format_hard_results) / self.n_samp
                    format_soft_avg = sum(format_soft_results) / self.n_samp
                else:
                    fact_results = factual_score_dataloader(
                        model=self.model,
                        tokenizer=self.tokenizer,
                        dataloader=self.dataloader,
                        n_samples=self.n_samp,
                    )
                factual_accuracy_avg = sum(fact_results) / self.n_samp

                if len(state.log_history) > 0:
                    state.log_history[-1]["factual_accuracy"] = factual_accuracy_avg
                    if self.output_format:
                        state.log_history[-1]["format_hard"] = format_hard_avg
                        state.log_history[-1]["format_soft"] = format_soft_avg
                else:
                    print("No log entries available to update.")

                time_taken = time.time() - start_time
                if self.verbose:
                    print(
                        f"[TIME] {time_taken:.2f} seconds: Model evaluated on FactualAccuracy."
                    )
            except Exception as e:
                print(f"Error during factual accuracy evaluation: {e}")



class FactualAccuracyCallback(TrainerCallback):
    """Un callback para evaluar y registrar la precisión factual del modelo durante el entrenamiento."""

    def __init__(
        self, model, tokenizer, df, n_samp=30, verbose=False, output_format=False
    ):
        super().__init__()
        self.model = model
        self.tokenizer = tokenizer
        self.df = df
        self.n_samp = n_samp
        self.verbose = verbose
        self.output_format = output_format

    def on_log(self, args, state, control, model=None, **kwargs):
        """Llamado después de registrar los últimos registros."""
        if model is not None:
            self.model = model
        elif self.model is None:
            print("Model is not available.")
            return

        if (
            state.is_local_process_zero
        ):  # Solo registrar desde el proceso principal cada 100 pasos
            start_time = time.time()
            try:
                with torch.no_grad():
                    if self.output_format:
                        fact_results, format_hard_results, format_soft_results = (
                            factual_score(
                                self.model,
                                self.tokenizer,
                                self.df,
                                n_samples=self.n_samp,
                                output_format=self.output_format,
                            )
                        )
                        # Calcular y registrar el resultado formateado
                        format_hard_avg = sum(format_hard_results) / self.n_samp
                        format_soft_avg = sum(format_soft_results) / self.n_samp
                    else:
                        fact_results = factual_score(
                            self.model,
                            self.tokenizer,
                            self.df,
                            n_samples=self.n_samp,
                            output_format=self.output_format,
                        )
                    factual_accuracy_avg = sum(fact_results) / self.n_samp

                if len(state.log_history) > 0:
                    state.log_history[-1]["factual_accuracy"] = factual_accuracy_avg
                    if self.output_format:
                        state.log_history[-1]["format_hard"] = format_hard_avg
                        state.log_history[-1]["format_soft"] = format_soft_avg
                else:
                    print("No log entries available to update.")

                time_taken = time.time() - start_time
                if self.verbose:
                    print(
                        f"[TIME] {time_taken:.2f} seconds: Model evaluated on FactualAccuracy."
                    )
            except Exception as e:
                print(f"Error during factual accuracy evaluation: {e}")


def fine_tuned_specific_layers(
    n_rows=1000,
    mod=4,
    model_name_or_path="EleutherAI/pythia-31M",
    tokenizer_name_or_path=None,
    learning_rate=5.0e-3,
    num_epochs=15,
    batch_size=32,
    save_dir="",
    verbose=True,  # Agregando el parámetro verbose aquí
    model_path=MODEL_PATH,
    special_format=True,
):
    # Inicializar una variable para llevar un registro del tiempo de inicio
    start_time = time.time()

    if verbose:
        print("Starting the dataset generation process.")

    gen_mod_dataset(
        n_rows=n_rows,
        mod=mod,
        lower_bound_gen=0,
        higher_bound_gen=100,
        special_format=special_format,
    )

    if verbose:
        print(
            f"[TIME] {time.time() - start_time:>8.2f} seconds: Dataset generation completed."
        )

    start_time = time.time()
    mod_dataset = format_and_load_mod_data(mod=mod)

    if tokenizer_name_or_path is None:
        tokenizer_name_or_path = model_name_or_path
        if verbose:
            print(
                f"No tokenizer specified, using the model path for tokenizer: {tokenizer_name_or_path}"
            )

    start_time = time.time()
    model = AutoModelForCausalLM.from_pretrained(model_name_or_path)
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)
    if verbose:
        print(
            f"[TIME] {time.time() - start_time:>8.2f} seconds: Loading model and tokenizer completed."
        )

    num_layers = model.config.num_hidden_layers

    layers = list(
        range(1, num_layers)
    )  # TODO: la última capa sola no puede funcionar, pero funciona conjuntamente con otras
    layer_combinations = []

    # Generar combinaciones contiguas de hasta 3 capas
    for i in range(len(layers)):
        for j in range(1, 4):  # 1, 2 o 3 capas
            if i + j <= len(layers):
                layer_combinations.append(layers[i : i + j])
    for layers in layer_combinations:
        start_time = time.time()
        footer = "layers_" + "_".join([str(x) for x in layers])

        save_path = os.path.join(model_path, save_dir)
        if not os.path.exists(save_path):
            os.makedirs(save_path)
            if verbose:
                print(f"Directory {save_path} created.")

        if verbose:
            print(f"Configuring fine-tuning for layer combination: {footer}")

        peft_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=32,
            lora_alpha=32,
            lora_dropout=0.1,
            layers_to_transform=layers,
        )

        model, tokenizer = initialize_model_and_tokenizer(
            model_name_or_path, tokenizer_name_or_path, config=peft_config
        )

        if verbose:
            print(
                f"[TIME] {time.time() - start_time:>8.2f} seconds: Model and tokenizer initialization completed."
            )

        start_time = time.time()
        eval_df = load_sample_data(mod=mod, n_samples=100)
        if verbose:
            print(
                f"[TIME] {time.time() - start_time:>8.2f} seconds: Evaluation sample data loaded."
            )

        model_name = model_name_or_path.split("/")[-1]
        training_args = setup_training_args(
            os.path.join(save_path, "checkpoints"),
            model_name,
            learning_rate=learning_rate,
            num_epochs=num_epochs,
            footer=footer,
            batch_size=batch_size,
            total_train_examples=n_rows,
        )

        start_time = time.time()
        if verbose:
            print("Starting model training.")
        trainer = train_model(
            mod_dataset,
            model,
            tokenizer,
            training_args,
            callbacks=[
                FactualAccuracyCallback(
                    model, tokenizer, eval_df, n_samp=100, output_format=True
                )
            ],
            verbose=verbose,
        )
        if verbose:
            print(
                f"[TIME] {time.time() - start_time:>8.2f}: Model training completed in  seconds."
            )

        model_save_name = f"{model_name}_trained_{footer}"
        save_path = os.path.join(save_path, model_save_name)
        if verbose:
            print(f"Saving model to {save_path}.")

        start_time = time.time()
        trainer.save_model(output_dir=save_path)
        if verbose:
            print(f"[TIME] {time.time() - start_time:>8.2f}: Model saved in  seconds.")

        # Acceder al historial de registros
        log_history = trainer.state.log_history

        # Guardar los registros de entrenamiento
        logs_save_path = os.path.join(save_path, "training_logs.json")
        with open(logs_save_path, "w") as f:
            json.dump(log_history, f, indent=4)

        if verbose:
            print(f"Training logs saved to {logs_save_path}.")

    # Fusionar modelos
    to_merge = [
        x
        for x in os.listdir(os.path.join(model_path, save_dir))
        if all([a not in x for a in ["_checkpoints", "merged"]])
    ]
    to_merge = [x for x in to_merge if "_trained_" in x]
    for model_name in to_merge:
        print("C", model_name)
        merge_lora_model(
            model_name=model_name,
            model_load_path=os.path.join(model_path, save_dir),
            model_save_path=os.path.join(model_path, save_dir),
        )
        print(model_name, "MERGED !")


def fine_tuned_simple(
    n_rows=1000,
    mod=4,
    model_name_or_path="EleutherAI/pythia-31M",
    tokenizer_name_or_path=None,
    learning_rate=5.0e-3,
    num_epochs=15,
    batch_size=32,
    save