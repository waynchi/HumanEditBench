import os
import random
import torch
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import precision_score, recall_score
from torch.nn import functional as F
from PIL import Image
import matplotlib.pyplot as plt
import seaborn as sns


# Ścieżka do wyodrębnionego zbioru danych Flickr8k
FLICKR8K_IMAGES_PATH = "flickr8k/Images"
FLICKR8K_CAPTIONS_PATH = "flickr8k/captions.txt"


# Funkcja do ładowania par obraz-tekst z Flickr8k
def load_flickr8k_data(images_path, captions_path, fraction=0.1):
    # Przeczytaj plik z podpisami
    with open(captions_path, "r") as f:
        captions_data = f.readlines()[1:]  # Pomiń nagłówek

    # Przetwórz podpisy
    image_text_pairs = {}
    for line in captions_data:
        image_name, caption = line.strip().split(",", 1)
        if image_name not in image_text_pairs:
            image_text_pairs[image_name] = []
        image_text_pairs[image_name].append(caption)

    # Załaduj tylko część zbioru danych
    selected_images = random.sample(
        list(image_text_pairs.keys()), int(len(image_text_pairs) * fraction)
    )
    image_text_pairs = {k: image_text_pairs[k] for k in selected_images}

    # Utwórz pary obrazów i podpisów
    pairs = []
    for image_name, captions in image_text_pairs.items():
        image_path = os.path.join(images_path, image_name)
        if os.path.exists(image_path):
            pairs.append((Image.open(image_path), random.choice(captions)))
    return pairs


# Funkcja do tworzenia niepowiązanych par
def create_unrelated_pairs(image_text_pairs):
    """Tworzy niepowiązane pary obrazów i tekstów poprzez losowe przetasowanie tekstów.

Argumenty:
    image_text_pairs (list): Lista krotek zawierających obrazy i ich odpowiadające teksty.

Zwraca:
    list: Lista krotek zawierających obrazy i niepowiązane teksty."""
    images, texts = zip(*image_text_pairs)
    unrelated_texts = random.sample(texts, len(texts))
    return list(zip(images, unrelated_texts))


def create_visual_pairs(image_text_pairs):
    """Tworzy pary oryginalnych i zaugmentowanych obrazów z par obraz-tekst.

Ta funkcja przyjmuje listę par obraz-tekst i tworzy nowe pary składające się z oryginalnych obrazów i ich zaugmentowanych wersji. Augmentacja użyta w tej implementacji to poziome odbicie.

Argumenty:
    image_text_pairs (list): Lista krotek zawierających pary (obraz, tekst), gdzie obrazy są obiektami PIL Image, a teksty są ciągami znaków.

Zwraca:
    list: Lista krotek zawierających pary (oryginalny_obraz, zaugmentowany_obraz), gdzie oba elementy są obiektami PIL Image."""
    from torchvision.transforms import ToTensor

    images, _ = zip(*image_text_pairs)
    augmented_images = [
        ToTensor()(image).flip(-1) for image in images
    ]  # Przykład augmentacji: poziome odbicie
    return list(zip(images, augmented_images))


def get_embeddings(images, texts, model_id="google/siglip-base-patch16-224"):
    """Dla podanych list obrazów i tekstów zwraca znormalizowane osadzenia dla obu."""
    # Upewnij się, że texts jest listą ciągów znaków
    if not all(isinstance(t, str) for t in texts):
        raise ValueError("All text inputs must be strings.")

    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = AutoModel.from_pretrained(model_id, ignore_mismatched_sizes=True).to(device)
    processor = AutoProcessor.from_pretrained(model_id)

    # Przetwarzaj obrazy i teksty
    image_inputs = processor(images=images, return_tensors="pt").to(device)
    text_inputs = processor(text=texts, return_tensors="pt", padding="max_length").to(
        device
    )

    with torch.no_grad():
        image_embeds = model.get_image_features(**image_inputs)
        text_embeds = model.get_text_features(**text_inputs)

    # Normalizuj osadzenia
    image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)
    text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)

    return image_embeds, text_embeds


def cosine_similarity_analysis(embeddings1, embeddings2, title):
    """Oblicza podobieństwo cosinusowe dla pasujących i niepowiązanych par oraz porównuje rozkłady."""
    similarities = cosine_similarity(
        embeddings1.cpu().numpy(), embeddings2.cpu().numpy()
    )

    # Dopasowane pary: Diagonalna macierzy podobieństwa
    matching_similarities = np.diag(similarities)

    # Niezwiązane pary: Podobieństwa poza przekątną
    unrelated_similarities = similarities[~np.eye(similarities.shape[0], dtype=bool)]

    print(f"### {title} ###")
    print(f"Mean Matching Similarity: {np.mean(matching_similarities):.4f}")
    print(f"Mean Unrelated Similarity: {np.mean(unrelated_similarities):.4f}")
    print()

    # Rysuj rozkłady
    plt.figure(figsize=(10, 6))
    sns.histplot(
        matching_similarities, kde=True, label="Matching Pairs", color="blue", bins=30
    )
    sns.histplot(
        unrelated_similarities, kde=True, label="Unrelated Pairs", color="red", bins=30
    )
    plt.title(f"{title}: Cosine Similarity Distributions")
    plt.xlabel("Cosine Similarity")
    plt.ylabel("Frequency")
    plt.legend()
    plt.show()


# ## b. Wyszukiwanie najbliższego sąsiada
def retrieval_metrics(query_embeds, target_embeds, ground_truth_indices, k=5):
    """Oblicza Precision@k i Recall@k dla wyszukiwania najbliższego sąsiada.

Ta funkcja ocenia skuteczność wyszukiwania poprzez obliczanie Precision@k i Recall@k.
Precision@k mierzy dokładność pierwszych k zwróconych elementów, podczas gdy Recall@k mierzy zdolność
do znalezienia odpowiedniego elementu wśród pierwszych k zwróconych elementów. Zakłada, że istnieje tylko jedno prawdziwe
dopasowanie na zapytanie.

Argumenty:
    query_embeds (torch.Tensor): Wektory osadzeń danych zapytania.
    target_embeds (torch.Tensor): Wektory osadzeń danych docelowych (baza danych).
    ground_truth_indices (list): Lista indeksów w danych docelowych reprezentujących prawdziwe dopasowania dla każdego zapytania.
    k (int): Liczba najlepszych wyników do rozważenia.

Zwraca:
    tuple: Krotka zawierająca średnie Precision@k i średnie Recall@k."""
    similarities = cosine_similarity(
        query_embeds.cpu().numpy(), target_embeds.cpu().numpy()
    )
    sorted_indices = np.argsort(-similarities, axis=1)[:, :k]  # Indeksy top-k

    # Oblicz metryki
    precisions = []
    recalls = []
    for i, true_idx in enumerate(ground_truth_indices):
        retrieved_indices = sorted_indices[i]
        true_positives = int(true_idx in retrieved_indices)
        precisions.append(true_positives / k)
        recalls.append(true_positives / 1)  # Tylko jedno prawdziwe dopasowanie na zapytanie

    mean_precision = np.mean(precisions)
    mean_recall = np.mean(recalls)

    return mean_precision, mean_recall


def plot_query_token_importance(
    pil_image, similarity_maps, query_tokens, alpha: float = 0.5
) -> None:
    """Rysuje oddzielną mapę cieplną dla każdego tokena zapytania w similarity_maps.

    Argumenty:
    pil_image (PIL.Image.Image): Oryginalny obraz (np. załadowany za pomocą Image.open(...)).
    similarity_maps (torch.Tensor):
        Kształt = (num_query_tokens, n_patches_x, n_patches_y).
    query_tokens (List[str]): Lista ciągów znaków dla każdego tokena w zapytaniu.
    alpha (float): Przezroczystość nakładek map cieplnych (0=przezroczysty, 1=nieprzezroczysty)."""
    # Konwertuj PIL na numpy
    image_np = np.array(pil_image)
    H, W = image_np.shape[:2]

    num_tokens = similarity_maps.size(0)
    assert num_tokens == len(query_tokens), (
        f"The number of query tokens in similarity_maps ({num_tokens}) "
        f"doesn't match the length of query_tokens list ({len(query_tokens)})."
    )

    fig, axs = plt.subplots(1, num_tokens, figsize=(5 * num_tokens, 5))
    if num_tokens == 1:
        # Jeśli jest tylko jeden token, axs nie będzie iterowalny
        axs = [axs]

    for idx in range(num_tokens):
        # Każda mapa podobieństwa dla pojedynczego tokenu zapytania: kształt = (n_patches_x, n_patches_y)
        single_map = similarity_maps[idx]  # (n_płatków_x, n_płatków_y)

        # Skaluj liniowo zamiast interpolować
        # Konwertuj tensor do numpy
        heatmap_np = single_map.cpu().numpy()
        
        # Skaluj liniowo do rozmiaru obrazu
        from torchvision.transforms.functional import resize
        import torch.nn.functional as F
        
        # Reshape do formatu (1, 1, H_map, W_map) aby użyć interpolate
        map_h, map_w = heatmap_np.shape
        heatmap_tensor = torch.tensor(heatmap_np).unsqueeze(0).unsqueeze(0)
        
        # Skaluj liniowo (nearest neighbor) do rozmiaru obrazu
        scaled_heatmap = F.interpolate(
            heatmap_tensor, 
            size=(H, W), 
            mode='nearest'
        )
        heatmap = scaled_heatmap.squeeze().numpy()

        # Wykres
        axs[idx].imshow(image_np, cmap=None if image_np.ndim == 3 else "gray")
        axs[idx].imshow(heatmap, cmap="jet", alpha=alpha)
        axs[idx].set_title(f"Query: {query_tokens[idx]}")
        axs[idx].axis("off")

    plt.tight_layout()
    plt.show()
