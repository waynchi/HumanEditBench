import pandas as pd
import os
import random
import torch
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import precision_score, recall_score
from torch.nn import functional as F
from PIL import Image, ImageDraw, ImageFont
import matplotlib.pyplot as plt
import seaborn as sns
from colpali_engine.interpretability import (
    get_similarity_maps_from_embeddings,
    plot_all_similarity_maps,
)


# Ruta al conjunto de datos Flickr8k extraído
FLICKR8K_IMAGES_PATH = "flickr8k/Images"
FLICKR8K_CAPTIONS_PATH = "flickr8k/captions.txt"

# Función para cargar pares de imagen-texto de Flickr8k


def load_flickr8k_data(images_path, captions_path, fraction=0.1):
    # Leer archivo de subtítulos
    with open(captions_path, "r") as f:
        captions_data = f.readlines()[1:]  # Omitir encabezado

    # Analizar subtítulos
    image_text_pairs = {}
    for line in captions_data:
        image_name, caption = line.strip().split(",", 1)
        if image_name not in image_text_pairs:
            image_text_pairs[image_name] = []
        image_text_pairs[image_name].append(caption)

    # Cargar solo una fracción del conjunto de datos
    selected_images = random.sample(
        list(image_text_pairs.keys()), int(len(image_text_pairs) * fraction)
    )
    image_text_pairs = {k: image_text_pairs[k] for k in selected_images}

    # Crear pares de imágenes y subtítulos
    pairs = []
    for image_name, captions in image_text_pairs.items():
        image_path = os.path.join(images_path, image_name)
        if os.path.exists(image_path):
            pairs.append((Image.open(image_path), random.choice(captions)))
    return pairs


# Función para crear pares no relacionados


def create_unrelated_pairs(image_text_pairs):
    """Crea pares no relacionados de imágenes y textos al mezclar aleatoriamente los textos.

Args:
    image_text_pairs (list): Una lista de tuplas que contiene imágenes y sus textos correspondientes.

Returns:
    list: Una lista de tuplas que contiene imágenes y textos no relacionados."""
    images, texts = zip(*image_text_pairs)
    unrelated_texts = random.sample(texts, len(texts))
    return list(zip(images, unrelated_texts))


def create_visual_pairs(image_text_pairs):
    """Crea pares de imágenes originales y aumentadas a partir de pares de imagen-texto.

Esta función toma una lista de pares de imagen-texto y crea nuevos pares que consisten
en las imágenes originales y sus versiones aumentadas. La augmentación utilizada
en esta implementación es un volteo horizontal.

Argumentos:
    image_text_pairs (list): Una lista de tuplas que contienen pares (imagen, texto),
        donde las imágenes son objetos PIL Image y los textos son cadenas de caracteres.

Devuelve:
    list: Una lista de tuplas que contienen pares (imagen_original, imagen_aumentada),
        donde ambos elementos son objetos PIL Image."""
    from torchvision.transforms import ToTensor

    images, _ = zip(*image_text_pairs)
    # Ejemplo de aumento: volteo horizontal
    augmented_images = [ToTensor()(image).flip(-1) for image in images]
    return list(zip(images, augmented_images))


def get_embeddings(images, texts, model_id="google/siglip-base-patch16-224"):
    """Dadas listas de imágenes y textos, devuelve embeddings normalizados para ambos."""
    # Asegúrate de que texts sea una lista de cadenas
    if not all(isinstance(t, str) for t in texts):
        raise ValueError("All text inputs must be strings.")

    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = AutoModel.from_pretrained(model_id, ignore_mismatched_sizes=True).to(device)
    processor = AutoProcessor.from_pretrained(model_id)

    # Preprocesar imágenes y textos
    image_inputs = processor(images=images, return_tensors="pt").to(device)
    text_inputs = processor(text=texts, return_tensors="pt", padding="max_length").to(
        device
    )

    with torch.no_grad():
        image_embeds = model.get_image_features(**image_inputs)
        text_embeds = model.get_text_features(**text_inputs)

    # Normalizar incrustaciones
    image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)
    text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)

    return image_embeds, text_embeds


def cosine_similarity_analysis(embeddings1, embeddings2, title):
    """Calcula la similitud coseno para pares coincidentes y no relacionados y compara distribuciones."""
    similarities = cosine_similarity(
        embeddings1.cpu().numpy(), embeddings2.cpu().numpy()
    )

    # Pares coincidentes: Diagonal de la matriz de similitud
    matching_similarities = np.diag(similarities)

    # Pares no relacionados: Similitudes fuera de la diagonal
    unrelated_similarities = similarities[~np.eye(similarities.shape[0], dtype=bool)]

    print(f"### {title} ###")
    print(f"Mean Matching Similarity: {np.mean(matching_similarities):.4f}")
    print(f"Mean Unrelated Similarity: {np.mean(unrelated_similarities):.4f}")
    print()

    # Graficar distribuciones
    plt.figure(figsize=(10, 6))
    sns.histplot(
        matching_similarities, kde=True, label="Matching Pairs", color="blue", bins=30
    )
    sns.histplot(
        unrelated_similarities, kde=True, label="Unrelated Pairs", color="red", bins=30
    )
    plt.title(f"{title}: Cosine Similarity Distributions")
    plt.xlabel("Cosine Similarity")
    plt.ylabel("Frequency")
    plt.legend()
    plt.show()


# b. Recuperación de Vecinos Más Cercanos


def retrieval_metrics(query_embeds, target_embeds, ground_truth_indices, k=5):
    """Calcula Precision@k y Recall@k para la recuperación de vecinos más cercanos.

Esta función evalúa la efectividad de la recuperación calculando Precision@k y Recall@k.
Precision@k mide la precisión de los elementos recuperados en el top-k, mientras que Recall@k mide la capacidad
de encontrar el elemento relevante dentro de los elementos recuperados en el top-k. Se asume que hay solo una coincidencia verdadera por consulta.

Args:
    query_embeds (torch.Tensor): Embeddings de los datos de consulta.
    target_embeds (torch.Tensor): Embeddings de los datos objetivo (base de datos).
    ground_truth_indices (list): Lista de índices en los datos objetivo que representan las coincidencias verdaderas para cada consulta.
    k (int): El número de mejores resultados a considerar.

Returns:
    tuple: Una tupla que contiene la media de Precision@k y la media de Recall@k."""
    similarities = cosine_similarity(
        query_embeds.cpu().numpy(), target_embeds.cpu().numpy()
    )
    sorted_indices = np.argsort(-similarities, axis=1)[:, :k]  # Índices de los k mejores

    # Calcular métricas
    precisions = []
    recalls = []
    for i, true_idx in enumerate(ground_truth_indices):
        retrieved_indices = sorted_indices[i]
        true_positives = int(true_idx in retrieved_indices)
        precisions.append(true_positives / k)
        recalls.append(true_positives / 1)  # Solo una coincidencia verdadera por consulta

    mean_precision = np.mean(precisions)
    mean_recall = np.mean(recalls)

    return mean_precision, mean_recall


def plot_query_token_importance(
    pil_image, similarity_maps, query_tokens, alpha: float = 0.5
) -> None:
    """Traza un mapa de calor separado para cada token de consulta en los similarity_maps.

Args:
    pil_image (PIL.Image.Image): La imagen original (por ejemplo, cargada a través de Image.open(...)).
    similarity_maps (torch.Tensor):
        Forma = (num_query_tokens, n_patches_x, n_patches_y).
    query_tokens (List[str]): Una lista de cadenas para cada token en la consulta.
    alpha (float): Transparencia para las superposiciones del mapa de calor (0=transparente, 1=opaco)."""
    # Convertir PIL a numpy
    image_np = np.array(pil_image)
    H, W = image_np.shape[:2]

    num_tokens = similarity_maps.size(0)
    assert num_tokens == len(query_tokens), (
        f"The number of query tokens in similarity_maps ({num_tokens}) "
        f"doesn't match the length of query_tokens list ({len(query_tokens)})."
    )

    fig, axs = plt.subplots(1, num_tokens, figsize=(5 * num_tokens, 5))
    if num_tokens == 1:
        # Si solo hay un token, axs no será un iterable
        axs = [axs]

    for idx in range(num_tokens):
        # Cada similarity_map para un solo token de consulta: forma = (n_patches_x, n_patches_y)
        single_map = similarity_maps[idx]  # (n_parches_x, n_patches_y)

        # Ampliar a tamaño completo de la imagen
        single_map_4d = single_map.unsqueeze(0).unsqueeze(
            0
        )  # (1,1,n_patches_x, n_patches_y)
        upsampled = F.interpolate(
            single_map_4d, size=(H, W), mode="bilinear", align_corners=False
        )

        # .to(torch.float32) arreglar si tu mapa es bfloat16
        heatmap = upsampled.squeeze().to(torch.float32).cpu().numpy()  # (H, W)

        # Opcionalmente normalizar el mapa de calor (descomentar si se desea)
        # heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-8)

        # Graficar
        axs[idx].imshow(image_np, cmap=None if image_np.ndim == 3 else "gray")
        axs[idx].imshow(heatmap, cmap="jet", alpha=alpha)
        axs[idx].set_title(f"Query: {query_tokens[idx]}")
        axs[idx].axis("off")

    plt.tight_layout()
    plt.show()


def get_maps_and_embeds(
    batch_images, batch_queries, model, processor, image, use_qwen=False
):
    """Calcula mapas de similitud y embeddings a partir de un lote de imágenes y consultas utilizando el modelo y procesador especificados.

Args:
    batch_images (dict): Un diccionario de entradas de imágenes en lotes procesadas por el procesador.
    batch_queries (dict): Un diccionario de entradas de consultas en lotes procesadas por el procesador.
    model (nn.Module): El modelo utilizado para calcular los embeddings.
    processor (Processor): El procesador responsable de la preprocesamiento de imágenes y texto.

Returns:
    tuple: Una tupla que contiene:
        - original_maps (torch.Tensor): Mapas de similitud entre imágenes y consultas
            con forma (num_queries, n_patches_x, n_patches_y).
        - original_image_embeddings (torch.Tensor): Embeddings de las imágenes de entrada.
        - original_query_embeddings (torch.Tensor): Embeddings de las consultas de entrada."""
    with torch.no_grad():
        original_image_embeddings = model.forward(**batch_images)
        original_query_embeddings = model.forward(**batch_queries)
    if use_qwen:
        n_patches = processor.get_n_patches(
            image_size=image.size,
            patch_size=model.patch_size,
            spatial_merge_size=model.spatial_merge_size,
        )
    else:
        n_patches = processor.get_n_patches(
            image_size=image.size, patch_size=model.patch_size
        )
    image_mask = processor.get_image_mask(batch_images)

    # Calcular mapas de similitud originales
    original_batched_maps = get_similarity_maps_from_embeddings(
        image_embeddings=original_image_embeddings,
        query_embeddings=original_query_embeddings,
        n_patches=n_patches,
        image_mask=image_mask,
    )
    # (longitud_consulta, n_parches_x, n_parches_y)
    original_maps = original_batched_maps[0].permute(0, 2, 1).contiguous()
    return original_maps, original_image_embeddings, original_query_embeddings


def visualize_token_map(image, original_maps, token_list, token_index=2, cmap="Greens", figsize=(15, 2), show_text=True):
    """Visualizar el mapa de atención de un token de tres maneras: la imagen original, el mapa de atención en bruto con valores numéricos, y una superposición del mapa de atención en la imagen original. Args: image (PIL.Image): La imagen de entrada para visualizar. original_maps (torch.Tensor o np.ndarray): Mapas de atención con forma (num_tokens, altura, ancho). token_list (list[str]): Lista de cadenas de tokens correspondientes a cada mapa de atención. token_index (int, opcional): Índice del token/mapa a visualizar. Por defecto es 2. cmap (str, opcional): Nombre del colormap de Matplotlib para visualizar los mapas de atención. Por defecto es "Greens". La función crea una figura con tres subgráficos: 1. La imagen de entrada original 2. El mapa de atención en bruto con valores numéricos anotados 3. El mapa de atención superpuesto en la imagen original con una barra de color Retorna: Ninguno. Muestra la visualización usando matplotlib."""
    # Convertir la imagen a un array de NumPy
    image_np = np.array(image)

    # Selecciona el mapa correspondiente al token
    visual_map = original_maps[token_index]

    # Convertir visual_map a un array de NumPy si es un tensor
    if isinstance(visual_map, torch.Tensor):
        visual_map = visual_map.cpu().to(dtype=torch.float32).numpy()
    elif not isinstance(visual_map, np.ndarray):
        visual_map = np.array(visual_map)

    # Convertir el mapa a una imagen PIL
    visual_map_pil = Image.fromarray(visual_map)

    # Redimensionar usando NEAREST para mantener "pixeles grandes
    visual_map_pil = visual_map_pil.resize(
        (image_np.shape[1], image_np.shape[0]),  # (ancho, alto)
        resample=Image.NEAREST,
    )

    # Convertir de nuevo a NumPy
    resized_map = np.array(visual_map_pil)

    # Crear una figura con subgráficas
    fig, axes = plt.subplots(1, 3, figsize=(15, 2))

    # Mostrar la imagen sin procesar
    axes[0].imshow(image_np)
    axes[0].set_title("Raw Image")
    axes[0].axis("off")
    # Mostrar el mapa sin procesar con anotaciones
    im = axes[1].imshow(visual_map, cmap=cmap)
    axes[1].set_title("Raw Map")
    axes[1].axis("off")

    if(show_text):
        # Anotar el mapa de calor
        for i in range(visual_map.shape[0]):
            for j in range(visual_map.shape[1]):
                text = axes[1].text(
                    j,
                    i,
                    f"{visual_map[i, j]:.2f}",
                    ha="center",
                    va="center",
                    color="w" if visual_map[i, j] > visual_map.max() / 2 else "black",
                )

    # Mostrar el gráfico superpuesto
    axes[2].imshow(image_np, alpha=1)
    axes[2].imshow(resized_map, cmap=cmap, alpha=0.6)
    axes[2].set_title("Overlay: Image + Map")
    axes[2].axis("off")
    # Agregar una barra de color para la superposición con valores coincidentes al mapa original
    cbar = fig.colorbar(
        plt.cm.ScalarMappable(
            cmap=cmap, norm=plt.Normalize(vmin=visual_map.min(), vmax=visual_map.max())
        ),
        ax=axes[2],
        shrink=0.8,
        orientation="vertical",
    )
    cbar.set_label("Map Intensity")
    # Agregar un título con el nombre del token
    plt.suptitle(f"Token: {token_list[token_index]}")

    # Ajustar diseño y mostrar
    plt.tight_layout()
    plt.show()


def create_single_patch_image(
    n_patches_x,
    n_patches_y,
    patch_size,
    main_color,
    special_color,
    special_patch,
    special_patch_width=2,
):
    """
    Creates an image composed of colored patches, with one special patch highlighted.

    The image is divided into a grid of n_patches_x by n_patches_y patches, each of size
    patch_size x patch_size pixels. All patches are filled with the main_color, except
    for the special_patch, which is filled with special_color.  The special patch can
    also have a width of more than one patch.
    Args:
        n_patches_x (int): Number of patches horizontally.
        n_patches_y (int): Number of patches vertically.
        patch_size (int): The size (in pixels) of each square patch.
        main_color (list): The [R, G, B] color for most patches.
        special_color (list): The [R, G, B] color for the special patch.
        special_patch (tuple): The (row, col) position of the top-left corner of the special patch (0-indexed).
        special_patch_width (int, optional): The width of the special patch in number of patches. Defaults to 2.

    Returns:
        PIL Image: The generated image.
    """

    # Crear un array 3D de NumPy para la imagen
    img_height = n_patches_y * patch_size
    img_width = n_patches_x * patch_size
    image_data = np.zeros((img_height, img_width, 3), dtype=np.uint8)

    # Rellenar toda la imagen con el color principal
    image_data[:, :] = main_color

    # Asignar el color especial al parche especial
    special_row, special_col = special_patch
    image_data[
        special_row * patch_size : (special_row + special_patch_width) * patch_size,
        special_col * patch_size : (special_col + special_patch_width) * patch_size,
    ] = special_color

    return Image.fromarray(image_data)


def extract_patch_mask(image, patch_size, special_color=[0, 0, 0]):
    """Extraer una máscara binaria que indique la ubicación del parche especial.

Args:
    image (PIL.Image.Image): La imagen de entrada.
    patch_size (int): El tamaño de cada parche cuadrado en píxeles.
    special_color (list[int]): El color RGB del parche especial.

Returns:
    np.ndarray: Una máscara binaria de forma (n_patches_y, n_patches_x) que indica
                la ubicación del parche especial (1 para parche especial, 0 de lo contrario)."""
    # Convertir la imagen a un array de NumPy
    image_np = np.array(image)

    # Obtener dimensiones de la imagen
    img_height, img_width, _ = image_np.shape

    # Calcular el número de parches
    n_patches_y = img_height // patch_size
    n_patches_x = img_width // patch_size

    # Inicializar la máscara de parches
    patch_mask = np.zeros((n_patches_y, n_patches_x), dtype=np.int32)

    # Iterar sobre todos los parches para localizar el parche especial
    for row in range(n_patches_y):
        for col in range(n_patches_x):
            # Extraer el parche
            patch = image_np[
                row * patch_size : (row + 1) * patch_size,
                col * patch_size : (col + 1) * patch_size,
            ]

            # Verificar si el parche coincide con el color especial
            if np.allclose(patch.mean(axis=(0, 1)), special_color, atol=1e-6):
                patch_mask[row, col] = 1  # Marcar este parche como especial

    return patch_mask


def evaluate_map_quality(similarity_map, patch_mask):
    """Evaluar la calidad de un mapa de similitud con respecto a una máscara binaria de parche.

Args:
    similarity_map (np.ndarray): El mapa de similitud (altura, ancho).
    patch_mask (np.ndarray): La máscara binaria para el parche (1 para parche negro, 0 en otros lugares).

Returns:
    dict: Métricas que incluyen correlación, precisión máxima y puntuación de superposición."""
    # Aplanar el mapa y la máscara para facilitar el cálculo
    sim_map_flat = similarity_map.flatten()
    patch_mask_flat = patch_mask.flatten()
    
    # (A) Correlación
    # Convertir a numpy array float32 para evitar el error de atributo 'astype'
    sim_map_flat_np = np.array(sim_map_flat, dtype=np.float32)
    patch_mask_flat_np = np.array(patch_mask_flat, dtype=np.float32)
    correlation = np.corrcoef(sim_map_flat_np, patch_mask_flat_np)[0, 1]
    
    # (B) Ubicación del Pico de Señal
    max_location = np.unravel_index(np.argmax(similarity_map), similarity_map.shape)
    expected_location = np.unravel_index(np.argmax(patch_mask), patch_mask.shape)
    peak_accuracy = 1 if max_location == expected_location else 0

    # (C) Superposición de Mapa Normalizado
    black_patch_score = similarity_map[patch_mask == 1].mean()
    background_score = similarity_map[patch_mask == 0].mean()
    overlap_score = black_patch_score / (
        background_score + 1e-8
    )  # Evitar la división por cero

    # Devuelve todas las métricas
    return {
        "correlation": correlation,
        "peak_accuracy": peak_accuracy,
        "overlap_score": overlap_score,
    }


def evaluate_image_maps(similarity_map, real_image):
    """
    Evaluates the quality of similarity maps by comparing them to a real image.

    This function assesses the alignment between a similarity map and a corresponding
    real image. It calculates several metrics:

    - Accuracy: Checks if any of the maximum values in the similarity map overlap with
      non-zero pixels in the real image (converted to grayscale).
    - Score: Computes a normalized score by summing the element-wise product of the
      similarity map and the normalized grayscale image, divided by the sum of the
      grayscale image pixel values.  This measures the weighted overlap, giving more
      importance to brighter regions in the real image.
    - Rank: Determines the rank of the average value within the special patch in the sorted
      list of all values in the similarity map. This indicates how strongly the map
      highlights the special patch compared to other regions.

    Args:
        similarity_map (np.ndarray): The similarity map to evaluate.
        real_image (PIL.Image.Image): The corresponding real image.

    Returns:
        dict: A dictionary containing the calculated metrics: accuracy, score, and rank.
    """
    # Convertir la imagen real a un arreglo binario (1 - escala de grises normalizada)
    image_array = 1 - np.array(real_image.convert("L"), dtype=np.float32) / 255.0

    # Crear una máscara para los valores máximos en el mapa de similitud
    acc_visual_map = np.where(similarity_map == similarity_map.max(), similarity_map, 0)
    visual_map = np.copy(similarity_map)

    # Verificar si es necesario escalar
    if image_array.shape != visual_map.shape:
        scale_factor = image_array.shape[0] // visual_map.shape[0]
        scaled_visual_map = np.kron(
            np.abs(visual_map), np.ones((scale_factor, scale_factor))
        )
        rank_map = np.kron(np.abs(visual_map), np.ones((scale_factor, scale_factor)))
        acc_visual_map = np.kron(
            np.abs(acc_visual_map), np.ones((scale_factor, scale_factor))
        )
    else:
        scaled_visual_map = visual_map

    # Calcular precisión y puntuación
    accuracy = np.any(image_array * acc_visual_map)
    score = np.sum(image_array * scaled_visual_map) / (
        np.sum(image_array) + 1e-8
    )  # Evitar la división por cero
    bin_image = (image_array != 0).astype(int)
    rank = np.sum(bin_image * rank_map) / np.sum(bin_image)  # Evitar la división por cero
    rank = np.where(
        np.isclose(sorted(list(np.abs(similarity_map.ravel())))[::-1], rank)
    )[0][0]

    return {
        "accuracy": accuracy,
        "score": score,
        "rank": rank,
    }


def create_single_patch_image_with_text(
    n_patches_x,
    n_patches_y,
    patch_size,
    main_color,
    special_color,
    special_patch,
    text="Hello",
    text_color=(255, 255, 255),
    special_patch_width=2,
    font_size=16,
    # Agregado parámetro font_path con valor por defecto
    font_path="./fonts/Roboto-Regular.ttf",
):
    """Crea una imagen compuesta de parches coloreados, pero coloca una sola palabra (o texto) dentro del área del parche 'especial'."""
    # Crear un array 3D de NumPy para la imagen
    img_height = n_patches_y * patch_size
    img_width = n_patches_x * patch_size
    image_data = np.zeros((img_height, img_width, 3), dtype=np.uint8)

    # Llenar toda la imagen con el color principal
    image_data[:, :] = main_color

    # Asignar el color especial al área del parche especial
    special_row, special_col = special_patch
    image_data[
        special_row * patch_size : (special_row + special_patch_width) * patch_size,
        special_col * patch_size : (special_col + special_patch_width) * patch_size,
    ] = special_color

    # Convertir a una imagen de Pillow para que podamos dibujar en ella
    img = Image.fromarray(image_data)
    draw = ImageDraw.Draw(img)

    # Cargar fuente con el tamaño especificado
    try:
        font = ImageFont.truetype(font_path, font_size)
    except IOError:
        print(f"Error loading font from {font_path}. Using default font.")
        font = ImageFont.load_default()

    # Calcular el centro del parche especial en coordenadas de píxeles
    patch_center_x = special_col * patch_size + (special_patch_width * patch_size) // 2
    patch_center_y = special_row * patch_size + (special_patch_width * patch_size) // 2

    # Calcular el cuadro delimitador del texto para centrar el texto
    text_bbox = draw.textbbox((0, 0), text, font=font)
    text_width = text_bbox[2] - text_bbox[0]
    text_height = text_bbox[3] - text_bbox[1]

    text_x = patch_center_x - text_width // 2
    text_y = patch_center_y - text_height // 2

    # Colocar el texto en el centro del parche especial
    draw.text((text_x, text_y), text, fill=text_color, font=font)

    return img


def visualize_results_grid(results_df):
    columns = [results_df.iloc[:, i] for i in range(len(results_df.columns))]
    columns = [
        (
            pd.to_numeric(col, errors="coerce")
            if not pd.api.types.is_numeric_dtype(col)
            else col
        )
        for col in columns
    ]

    # Deducir la forma de la cuadrícula a partir del número de filas de resultados
    grid_size = int(np.sqrt(len(results_df)))
    # Reformar columnas en matrices
    matrices = [col.to_numpy().reshape(grid_size, grid_size) for col in columns]

    # Configuración de visualización
    fig, axes = plt.subplots(1, len(results_df.columns), figsize=(12, 2))
    titles = [
        (
            f"{results_df.columns[i]} (Categorical/Binary)"
            if i == 0
            else f"{results_df.columns[i]} (Continuous)"
        )
        for i in range(len(results_df.columns))
    ]
    # Añadido mapa de colores para el cuarto gráfico
    cmaps = ["coolwarm"] * len(results_df.columns)
    # Graficar cada matriz
    for i, (matrix, ax, title, cmap) in enumerate(zip(matrices, axes, titles, cmaps)):
        im = ax.imshow(matrix, cmap=cmap, interpolation="none")
        ax.set_title(title)
        ax.set_xticks(range(grid_size))
        ax.set_yticks(range(grid_size))
        fig.colorbar(im, ax=ax)

    # Mostrar el gráfico
    plt.tight_layout()
    plt.show()



def run_expe_word_square(
    word_to_write,
    token,
    n_patches_x,
    n_patches_y,
    patch_size,
    model,
    processor,
    device,
    use_qwen,
    main_color=[255, 255, 255],
    special_color=(0, 0, 0),
):

    all_images_text = [
        create_single_patch_image_with_text(
            n_patches_x=n_patches_x,
            n_patches_y=n_patches_y,
            patch_size=patch_size,
            main_color=main_color,
            special_color=main_color,
            special_patch=(row, col),
            text=word_to_write,
            text_color=(0,0,0),  # color_texto,
            font_size=9,
        )
        for row in range(0, n_patches_y, 2)
        for col in range(0, n_patches_x, 2)
    ]

    all_maps = []
    for image in all_images_text:
        batch_images = processor.process_images([image]).to(device)
        batch_queries = processor.process_queries([token]).to(device)
        original_maps, original_image_embeddings, original_query_embeddings = (
            get_maps_and_embeds(
                batch_images, batch_queries, model, processor, image, use_qwen=use_qwen
            )
        )
        original_maps = original_maps.to(dtype=torch.float32).cpu().numpy()
        all_maps.append(original_maps)

    input_ids = batch_queries["input_ids"][0]  # forma: (num_subtokens,)
    token_list = [processor.tokenizer.decode([token_id]) for token_id in input_ids]
    # imprimir(token_list)
    indexes = [i for i, x in enumerate(token_list) if "<" not in x and ">" not in x][2:]
    # imprimir(indexes)
    # imprimir(np.array(token_list)[[indexes]])

    results_df = pd.DataFrame(columns=["accuracy", "score", "rank"])
    for i, (this_map, image) in enumerate(zip(all_maps, all_images_text)):
        visual_map = this_map[token_index]
        patch_mask = extract_patch_mask(image, patch_size)
        metrics = evaluate_map_quality(visual_map, patch_mask)
        results_df.loc[i] = metrics.values()
    return results_df