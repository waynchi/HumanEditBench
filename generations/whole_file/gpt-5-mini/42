import logging
import os
from typing import Any, Dict, List
from pydantic import BaseModel, Field
from carvana_enzo_worker.enums.gpt_enums import GptModels, VertextAIModels
from carvana_enzo_worker.providers.vertexai_claude_provider import VertexAIClaudeProvider
from carvana_enzo_worker.providers.vertexai_gemini_provider import VertexAIGeminiProvider
from carvana_enzo_worker.providers.azure_o1_provider import AzureOpenAIo1Provider
from carvana_enzo_worker.providers.azure_gpt_provider import AzureOpenAIChatProvider

# pylint: disable=W1203, C0415 [Use %s formatting in logging function, import-outside-toplevel]


class LLMArena(BaseModel):
    """
    A tool to generate chats using multiple LLM's for a given prompt
    """

    prompt: str = Field(..., description="The input prompt for the LLMs.")
    models: List[str] = Field(..., description="A list of model names to use for generating chats.")
    responses: List[str] = Field([], description="A list of generated chat responses.")
    kwargs: Dict[str, Any] = Field({}, description="Additional keyword arguments for the LLMs.")


    @staticmethod
    async def generate_responses_for_models(prompt: str, models: List[str], **kwargs: Any) -> List[str]:
        """
        Generate responses from multiple models for a given prompt.

        :param prompt: The input prompt for the LLMs.
        :param models: A list of model names to use for generating responses.
        :return: A list of generated responses.
        """
        responses = []
        providers = []
        for model in models:
            provider_for_model = LLMArena._get_provider_for_model(model, **kwargs)
            providers.append(provider_for_model)

        
        import asyncio

        # Run all provider.generate_chat_response coroutines in parallel
        tasks = []
        for provider in providers:
            try:
                coro = provider.generate_chat_response(prompt)
                tasks.append(asyncio.create_task(coro))
            except Exception as e:
                logging.error("Error scheduling response generation for %s: %s", provider, e)
                # preserve order: append a completed future-like exception placeholder
                # We'll handle by appending the error string directly to responses and adding None to tasks
                responses.append(f"Error scheduling response generation for {provider}: {e}")
                tasks.append(None)

        # Gather results for the tasks that were properly scheduled
        gather_tasks = [t for t in tasks if t is not None]
        results = []
        if gather_tasks:
            gathered = await asyncio.gather(*gather_tasks, return_exceptions=True)
            # Reconstruct results aligning with providers/tasks, inserting None for placeholders
            it = iter(gathered)
            for t in tasks:
                if t is None:
                    results.append(None)
                else:
                    results.append(next(it))

        # Process results and fill responses maintaining the original provider order
        for provider, result in zip(providers, results):
            if result is None:
                # Error was already logged and added to responses during scheduling
                # But we need to ensure the responses list aligns with providers
                # If the scheduling error already appended an entry, skip adding again.
                # Determine if we've already appended by checking count
                # Count current responses for this provider index:
                # We maintain that responses appended during scheduling are at same positions as providers with None tasks.
                # To keep it simple, if scheduling failed we already appended an error, so continue.
                continue

            if isinstance(result, Exception):
                logging.error("Error generating response from %s: %s", provider, result)
                responses.append(f"Error generating response from {provider}: {result}")
            else:
                responses.append(result)

        return responses
    

    @staticmethod
    def _get_provider_for_model(model: str, **kwargs: Any) -> Any:
        event_id = event_id = kwargs.get("event_id", "")

        if model == VertextAIModels.CLAUDE_3_5_SONNET_V2.name:
            return  VertexAIClaudeProvider(event_id=event_id, location=str(os.getenv("VERTEXAI_CLAUDE_REGION")), deployment_id=model)
        
        if model == VertextAIModels.GEMINI_2_0_FLASH_EXP.name:
            return VertexAIGeminiProvider(event_id=event_id, location=str(os.getenv("VERTEXAI_GEMINI_REGION")), deployment_id=model)
        
        if model == GptModels.o1.value:
            return AzureOpenAIo1Provider(event_id=event_id, deployment_id=model)
        
        return AzureOpenAIChatProvider(event_id=event_id, deployment_id=model)