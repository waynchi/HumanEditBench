from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Crear una sesión Spark
spark = SparkSession.builder.appName("EscrituraParquetADL2").getOrCreate()

# Ejemplo de datos (reemplazar con tus datos reales)
data = [
    {"id": 1, "nombre": "Juan", "edad": 30},
    {"id": 2, "nombre": "Ana", "edad": 25},
    {"id": 3, "nombre": "Pedro", "edad": 40}
]

# Crear un DataFrame a partir de los datos
import requests
import json

# URL de la API (reemplazar con la URL real)
api_url = "<your_api_endpoint>"

try:
    response = requests.get(api_url, timeout=60)
    response.raise_for_status()
    api_payload = response.json()
except Exception as e:
    raise RuntimeError(f"Error al obtener datos de la API: {e}")

print("Tipo de datos recibidos desde la API:", type(api_payload))

# Normalizar la respuesta para obtener una lista de registros (list[dict])
records = []
if isinstance(api_payload, list):
    records = api_payload
elif isinstance(api_payload, dict):
    # Intentar extraer listas comunes dentro del dict
    for possible_key in ("data", "results", "items", "records"):
        if possible_key in api_payload and isinstance(api_payload[possible_key], list):
            records = api_payload[possible_key]
            break
    if not records:
        # Si es un solo objeto, lo envolvemos en una lista
        records = [api_payload]
else:
    raise ValueError("Formato inesperado de la respuesta de la API. Se esperaba dict o list.")

if not records:
    raise ValueError("No se encontraron registros en la respuesta de la API.")

print(f"Número de registros extraídos: {len(records)}")
print("Ejemplo de registro (primer elemento):")
print(json.dumps(records[0], ensure_ascii=False, indent=2))

# Crear un DataFrame con Spark dejando que Spark infiera el esquema.
# Convertimos cada registro a una cadena JSON y usamos spark.read.json sobre un RDD.
rdd = spark.sparkContext.parallelize([json.dumps(rec) for rec in records])
df = spark.read.json(rdd)

print("Esquema inferido por Spark:")
df.printSchema()
print("Muestra de los datos:")
df.show(5, truncate=False)

# Configurar la conexión a ADL2 usando la identidad de Microsoft ID
# No es necesario proporcionar credenciales explícitamente en un notebook de Synapse
# Spark utilizará la identidad administrada del notebook para autenticarse.

# Especificar la ruta al contenedor y la carpeta en ADL2
container_name = "<your_container_name>"  # Reemplazar con el nombre de tu contenedor
folder_path = "<your_folder_path>"  # Reemplazar con la ruta a la carpeta dentro del contenedor
adl2_path = f"abfss://{container_name}@{<your_storage_account_name>}.dfs.core.windows.net/{folder_path}"

# Escribir el DataFrame en formato parquet en ADL2
df.write.parquet(adl2_path, mode="overwrite")

# Opcional: leer el archivo parquet para verificar
df_leido = spark.read.parquet(adl2_path)
df_leido.show()

# Detener la sesión Spark
spark.stop()