from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Crear una sesión Spark
spark = SparkSession.builder.appName("EscrituraParquetADL2").getOrCreate()

# Analizar los datos de la API antes de crear el DataFrame
# Primero necesitamos obtener los datos de la API (esto es un ejemplo, reemplaza con tu implementación real)
# Por ejemplo, usando requests o alguna otra librería para consumir APIs
import requests

# URL de la API (reemplazar con la URL real)
api_url = "https://api.ejemplo.com/datos"

# Hacer la solicitud GET a la API
response = requests.get(api_url)

# Verificar si la solicitud fue exitosa
if response.status_code == 200:
    # Analizar los datos JSON de la respuesta
    data = response.json()

    # Aquí puedes analizar la estructura de los datos antes de crear el DataFrame
    # Por ejemplo, imprimir las primeras filas para entender la estructura
    print("Estructura de los datos recibidos:")
    print(data[:3])  # Imprimir las primeras 3 entradas para revisar

    # Crear un DataFrame a partir de los datos
    df = spark.createDataFrame(data)
else:
    print(f"Error al obtener datos de la API. Código de estado: {response.status_code}")
    data = []  # Asignar una lista vacía si hay error

# Configurar la conexión a ADL2 usando la identidad de Microsoft ID
# No es necesario proporcionar credenciales explícitamente en un notebook de Synapse
# Spark utilizará la identidad administrada del notebook para autenticarse.

# Especificar la ruta al contenedor y la carpeta en ADL2
container_name = "<your_container_name>"  # Reemplazar con el nombre de tu contenedor
folder_path = "<your_folder_path>"  # Reemplazar con la ruta a la carpeta dentro del contenedor
adl2_path = f"abfss://{container_name}@{<your_storage_account_name>}.dfs.core.windows.net/{folder_path}"

# Escribir el DataFrame en formato parquet en ADL2
df.write.parquet(adl2_path, mode="overwrite")

# Opcional: leer el archivo parquet para verificar
df_leido = spark.read.parquet(adl2_path)
df_leido.show()

# Detener la sesión Spark
spark.stop()