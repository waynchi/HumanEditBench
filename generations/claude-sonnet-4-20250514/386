import os
import shutil

from transformers import AutoModelForCausalLM
from peft import PeftModel

from dotenv import load_dotenv

import pickle
import torch
import json

load_dotenv()

DATA_SAVE_PATH = os.getenv("DATA_SAVE_PATH")
MODEL_PATH = os.getenv("MODEL_PATH")


def save_log_to_file(log_history, file_path, append_latest_only=False):
    """Zapisuje historię logów do pliku JSON. Jeśli plik już istnieje, dodaje do niego.

Parametry:
- log_history: Lista wpisów logów (każdy wpis to słownik).
- file_path: Ścieżka do pliku, w którym logi będą zapisywane.
- append_latest_only: Jeśli True, tylko najnowszy wpis logu jest dodawany."""
    # Inicjalizuj current_logs
    current_logs = []

    # Jeśli plik istnieje, załaduj bieżące logi i dołącz do nich
    if os.path.exists(file_path):
        try:
            with open(file_path, "r") as f:
                content = f.read().strip()
                if content:
                    current_logs = json.loads(content)
                else:
                    current_logs = []
        except json.JSONDecodeError:
            print(f"Warning: {file_path} contains invalid JSON. Overwriting file.")
            current_logs = []
        except Exception as e:
            print(f"An error occurred while reading {file_path}: {e}")
            current_logs = []
    else:
        # Plik nie istnieje; current_logs pozostaje pustą listą
        pass

    # Zdecyduj, czy dołączyć całą historię logów, czy tylko najnowszy wpis
    if append_latest_only and log_history:
        # Dodaj tylko najnowszy log epoki
        current_logs.append(log_history[-1])
    else:
        # Dołącz całą historię logów
        current_logs.extend(log_history)

    # Zapisz zaktualizowaną historię logów
    try:
        with open(file_path, "w") as f:
            json.dump(current_logs, f, indent=4)
    except Exception as e:
        print(f"An error occurred while writing to {file_path}: {e}")

def clear_directory(directory, delete_directory=False, verbose=True):
    """Czyści wszystkie pliki i podkatalogi w podanym katalogu. Opcjonalnie usuwa sam katalog. Tworzy katalog, jeśli nie istnieje i delete_directory jest False.

Argumenty:
    directory (str): Ścieżka do katalogu do wyczyszczenia.
    delete_directory (bool): Jeśli True, usuwa katalog po wyczyszczeniu jego zawartości. Domyślnie False.
    verbose (bool): Jeśli False, wyłącza komunikaty debug. Domyślnie True.

Wyjątki:
    OSError: Jeśli wystąpi jakikolwiek błąd podczas usuwania plików lub katalogów. Dostarcza szczegóły dotyczące niepowodzenia.
Przykład:
    clear_directory('/path/to/my/directory')
    clear_directory('/path/to/my/directory', delete_directory=True)
    
Optymalizacje zaimplementowane:
1. Batch print statements - reduced I/O operations by collecting messages and printing once
2. Reduced redundant system calls by combining os.path.isdir with scandir for better performance
3. Added verbose flag to disable debug output when not needed
4. Optimized error handling to avoid redundant exception processing
"""
    if not os.path.exists(directory):
        if not delete_directory:
            os.makedirs(directory, exist_ok=True)
            if verbose:
                print(f"Directory '{directory}' created.")
        else:
            raise ValueError("Directory does not exist and delete_directory is True. Cannot proceed.")
        return

    # Use scandir for better performance on large directories
    # and batch print statements to reduce I/O overhead
    removed_items = []
    failed_items = []
    
    try:
        with os.scandir(directory) as entries:
            for entry in entries:
                try:
                    if entry.is_dir():
                        shutil.rmtree(entry.path)
                        if verbose:
                            removed_items.append(f"Removed directory: {entry.path}")
                    else:
                        os.remove(entry.path)
                        if verbose:
                            removed_items.append(f"Removed file: {entry.path}")
                except OSError as e:
                    error_msg = f"Failed to delete '{entry.path}'. Reason: {e}"
                    failed_items.append(error_msg)
                    if verbose:
                        print(error_msg)
                    raise  # Re-raise the exception to halt execution if a deletion fails
    except OSError as e:
        if verbose:
            print(f"Failed to access directory '{directory}'. Reason: {e}")
        raise

    # Batch print all success messages at once
    if verbose and removed_items:
        print('\n'.join(removed_items))

    if delete_directory:
        try:
            os.rmdir(directory)
            if verbose:
                print(f"Removed directory: {directory}")
        except OSError as e:
            if verbose:
                print(f"Failed to delete '{directory}'. Reason: {e}")
            raise  # Re-raise the exception to halt execution if directory removal fails


def merge_lora_model(
    model_name="pythia-31M",
    base_model_repo_name="EleutherAI/",
    model_load_path=MODEL_PATH,
    model_save_path=MODEL_PATH,
):

    my_model_path = os.path.join(model_load_path, model_name)
    param_count = model_name.lower().split("m")[0].split("-")[1]
    base_model = f"pythia-{param_count}M"

    base_model = AutoModelForCausalLM.from_pretrained(
        os.path.join(base_model_repo_name, base_model)
    )
    model = PeftModel.from_pretrained(base_model, my_model_path)
    merged_model = model.merge_and_unload()
    my_model_save_path = os.path.join(model_save_path, f"{model_name}_merged")
    merged_model.save_pretrained(my_model_save_path)


def remove_repetition(question, answer):
    if question in answer:
        return answer.replace(question, "").strip()
    return answer


def load_model(
    model_type,
    model_path=None,
    blocks_str=None,
    vanilla_model_name=None,
    host_model_name=None,
):
    """
    Loads different types of models based on the model_type parameter.

    Parameters:
    model_type (str): The type of model to load. One of 'Tuned Model', 'Vanilla Model',
                      'Transformed Model', 'Final Model', or 'Host Model'.
    model_path (str): The base path where models are stored.
    blocks_str (str): A string representing the layers or blocks used in model naming.
    vanilla_model_name (str): The name or path of the vanilla (base) model.
    host_model_name (str): The name or path of the host model.

    Returns:
    model: The loaded model object.

    Raises:
    ValueError: If an unknown model_type is provided or required parameters are missing.
    IOError: If loading the model fails.

    Example:
    model = load_model(
        model_type="Tuned Model",
        model_path="/path/to/models",
        blocks_str="1-5",
        vanilla_model_name="EleutherAI/pythia-31M"
    )
    """
    if model_type == "Tuned Model":
        model_name = vanilla_model_name.split("/")[-1]

        # save_path = os.path.join(model_path)
        # model_save_name = f"{model_name}_trained_{footer}
        # save_path = os.path.join(save_path, model_save_name)

        tuned_model_name = f"{model_name}_trained_layers_{blocks_str}_merged"
        tuned_model = AutoModelForCausalLM.from_pretrained(
            os.path.join(model_path, f"{tuned_model_name}")
        )
        return tuned_model

    elif model_type == "Vanilla Model":
        vanilla_model = AutoModelForCausalLM.from_pretrained(vanilla_model_name)
        return vanilla_model

    elif model_type == "Transformed Model":
        name = host_model_name.split("/")[-1]
        save_path = os.path.join(model_path, f"{name}_preGRAFTED_{blocks_str}.pkl")
        with open(save_path, "rb") as f:
            transformed_model = pickle.load(f)
        return transformed_model

    elif model_type == "Final Model":
        name = host_model_name.split("/")[-1]
        model_save_name = f"{name}_GRAFTED_{blocks_str}.pkl"
        save_path = os.path.join(model_path, model_save_name)
        with open(save_path, "rb") as f:
            final_model = pickle.load(f)
        return final_model
    elif model_type == "Host Model":
        host_model = AutoModelForCausalLM.from_pretrained(host_model_name)
        return host_model

    else:
        raise ValueError(f"Unknown model type: {model_type}")


def load_batch_losses(file_path):
    """Ładuje dane strat partii z pliku punktu kontrolnego.

Parametry:
file_path (str): Ścieżka do pliku punktu kontrolnego.

Zwraca:
list lub None: Straty partii, jeśli dostępne, w przeciwnym razie None.

Logi:
Komunikat o błędzie, jeśli ładowanie się nie powiedzie.

Przykład:
batch_losses = load_batch_losses('/path/to/checkpoint.pt')"""
    try:
        checkpoint = torch.load(file_path, map_location=torch.device("cpu"))
        batch_losses = checkpoint.get("batch_losses", None)
        if batch_losses is not None:
            logging.info(f"Batch losses loaded from {file_path}")
        else:
            logging.warning(f"No 'batch_losses' key found in checkpoint at {file_path}")
        return batch_losses
    except (FileNotFoundError, IOError, RuntimeError) as e:
        logging.error(f"Error loading checkpoint from {file_path}: {e}")
        return None