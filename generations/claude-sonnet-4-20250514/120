from pyspark.sql import SparkSession
from pyspark.sql.functions import col
import requests
import json

# Create a Spark session
spark = SparkSession.builder.appName("EscrituraParquetADL2").getOrCreate()

# Get data from API and analyze it
api_url = "<your_api_endpoint>"  # Replace with your actual API endpoint
response = requests.get(api_url)

if response.status_code == 200:
    api_data = response.json()
    
    # Analyze the API data structure
    print("API Response Analysis:")
    print(f"Data type: {type(api_data)}")
    print(f"Sample data structure: {json.dumps(api_data[:2] if isinstance(api_data, list) else api_data, indent=2)}")
    
    # Create DataFrame from API data
    if isinstance(api_data, list):
        df = spark.createDataFrame(api_data)
    else:
        # If it's a single object, wrap it in a list
        df = spark.createDataFrame([api_data])
    
    print("DataFrame Schema:")
    df.printSchema()
    print("Sample DataFrame content:")
    df.show()
    
else:
    print(f"Failed to fetch data from API. Status code: {response.status_code}")
    # Fallback to example data for testing
    data = [
        {"id": 1, "nombre": "Juan", "edad": 30},
        {"id": 2, "nombre": "Ana", "edad": 25},
        {"id": 3, "nombre": "Pedro", "edad": 40}
    ]
    df = spark.createDataFrame(data)

# Set up the connection to ADL2 using the Microsoft ID identity
# It is not necessary to provide credentials explicitly in a Synapse notebook
# Spark will use the managed identity of the notebook to authenticate.

# Specify the path to the container and folder in ADL2
container_name = "<your_container_name>"  # Replace with the name of your container
folder_path = "<your_folder_path>"  # Replace with the path to the folder within the container
adl2_path = f"abfss://{container_name}@{<your_storage_account_name>}.dfs.core.windows.net/{folder_path}"

# Write the DataFrame in parquet format to ADL2
df.write.parquet(adl2_path, mode="overwrite")

# Optional: read the parquet file to verify
df_leido = spark.read.parquet(adl2_path)
df_leido.show()

# Stop the Spark session
spark.stop()