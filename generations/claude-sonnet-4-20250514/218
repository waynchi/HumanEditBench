import os
import random
import torch
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import precision_score, recall_score
from torch.nn import functional as F
from PIL import Image
import matplotlib.pyplot as plt
import seaborn as sns

# 提取的Flickr8k数据集的路径
FLICKR8K_IMAGES_PATH = "flickr8k/Images"
FLICKR8K_CAPTIONS_PATH = "flickr8k/captions.txt"

# 从Flickr8k加载图像-文本对的函数
def load_flickr8k_data(images_path, captions_path, fraction=0.1):
    # 读取字幕文件
    with open(captions_path, "r") as f:
        captions_data = f.readlines()[1:]  # 跳过标题

    # 解析标题
    image_text_pairs = {}
    for line in captions_data:
        image_name, caption = line.strip().split(",", 1)
        if image_name not in image_text_pairs:
            image_text_pairs[image_name] = []
        image_text_pairs[image_name].append(caption)

    # 仅加载数据集的一部分
    selected_images = random.sample(list(image_text_pairs.keys()), int(len(image_text_pairs) * fraction))
    image_text_pairs = {k: image_text_pairs[k] for k in selected_images}

    # 创建图像和标题的配对
    pairs = []
    for image_name, captions in image_text_pairs.items():
        image_path = os.path.join(images_path, image_name)
        if os.path.exists(image_path):
            pairs.append((Image.open(image_path), random.choice(captions)))
    return pairs

# 创建不相关对的函数
def create_unrelated_pairs(image_text_pairs):
    """创建不相关的图像和文本对，通过随机打乱文本。

参数:
    image_text_pairs (list): 包含图像及其对应文本的元组列表。

返回:
    list: 包含图像和不相关文本的元组列表。"""
    images, texts = zip(*image_text_pairs)
    unrelated_texts = random.sample(texts, len(texts))
    return list(zip(images, unrelated_texts))


def create_visual_pairs(image_text_pairs):
    """从图像-文本对中创建原始图像和增强图像的对。

此函数接受一个图像-文本对的列表，并创建由原始图像及其增强版本组成的新对。此实现中使用的增强是水平翻转。

参数:
    image_text_pairs (list): 包含 (image, text) 对的元组列表，其中图像是 PIL Image 对象，文本是字符串。

返回:
    list: 包含 (original_image, augmented_image) 对的元组列表，其中两个元素都是 PIL Image 对象。"""
    from torchvision.transforms import ToTensor
    images, _ = zip(*image_text_pairs)
    augmented_images = [ToTensor()(image).flip(-1) for image in images]  # 示例增强：水平翻转
    return list(zip(images, augmented_images))


def get_embeddings(images, texts, model_id="google/siglip-base-patch16-224"):
    """给定图像和文本列表，返回两者的归一化嵌入。"""
    # 确保 texts 是字符串列表
    if not all(isinstance(t, str) for t in texts):
        raise ValueError("All text inputs must be strings.")

    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = AutoModel.from_pretrained(model_id, ignore_mismatched_sizes=True).to(device)
    processor = AutoProcessor.from_pretrained(model_id)
    
    # 预处理图像和文本
    image_inputs = processor(images=images, return_tensors="pt").to(device)
    text_inputs = processor(text=texts, return_tensors="pt", padding="max_length").to(device)
    
    with torch.no_grad():
        image_embeds = model.get_image_features(**image_inputs)
        text_embeds = model.get_text_features(**text_inputs)

    # 规范化嵌入
    image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)
    text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)

    return image_embeds, text_embeds


def cosine_similarity_analysis(embeddings1, embeddings2, title):
    """计算匹配和不相关对的余弦相似度并比较分布。"""
    similarities = cosine_similarity(embeddings1.cpu().numpy(), embeddings2.cpu().numpy())

    # 匹配对：相似度矩阵的对角线
    matching_similarities = np.diag(similarities)

    # 不相关的对：非对角线相似度
    unrelated_similarities = similarities[~np.eye(similarities.shape[0], dtype=bool)]

    print(f"### {title} ###")
    print(f"Mean Matching Similarity: {np.mean(matching_similarities):.4f}")
    print(f"Mean Unrelated Similarity: {np.mean(unrelated_similarities):.4f}")
    print()

    # 绘制分布图
    plt.figure(figsize=(10, 6))
    sns.histplot(matching_similarities, kde=True, label="Matching Pairs", color="blue", bins=30)
    sns.histplot(unrelated_similarities, kde=True, label="Unrelated Pairs", color="red", bins=30)
    plt.title(f"{title}: Cosine Similarity Distributions")
    plt.xlabel("Cosine Similarity")
    plt.ylabel("Frequency")
    plt.legend()
    plt.show()

# ## b. 最近邻检索
def retrieval_metrics(query_embeds, target_embeds, ground_truth_indices, k=5):
    """计算最近邻检索的Precision@k和Recall@k。

此函数通过计算Precision@k和Recall@k来评估检索的有效性。Precision@k衡量前k个检索项的准确性，而Recall@k衡量在前k个检索项中找到相关项的能力。假设每个查询只有一个真实匹配。

参数：
    query_embeds (torch.Tensor): 查询数据的嵌入。
    target_embeds (torch.Tensor): 目标数据（数据库）的嵌入。
    ground_truth_indices (list): 目标数据中表示每个查询的真实匹配的索引列表。
    k (int): 要考虑的前k个结果的数量。

返回：
    tuple: 包含平均Precision@k和平均Recall@k的元组。"""
    similarities = cosine_similarity(query_embeds.cpu().numpy(), target_embeds.cpu().numpy())
    sorted_indices = np.argsort(-similarities, axis=1)[:, :k]  # 前k个索引

    # 计算指标
    precisions = []
    recalls = []
    for i, true_idx in enumerate(ground_truth_indices):
        retrieved_indices = sorted_indices[i]
        true_positives = int(true_idx in retrieved_indices)
        precisions.append(true_positives / k)
        recalls.append(true_positives / 1)  # 每个查询只有一个真实匹配

    mean_precision = np.mean(precisions)
    mean_recall = np.mean(recalls)

    return mean_precision, mean_recall

def plot_query_token_importance(
    pil_image,
    similarity_maps,
    query_tokens,
    alpha: float = 0.5
) -> None:
    """为similarity_maps中的每个查询标记绘制单独的热图。

参数：
    pil_image (PIL.Image.Image): 原始图像（例如，通过Image.open(...)加载）。
    similarity_maps (torch.Tensor): 
        形状 = (num_query_tokens, n_patches_x, n_patches_y)。
    query_tokens (List[str]): 查询中每个标记的字符串列表。
    alpha (float): 热图叠加的透明度（0=透明，1=不透明）。"""
    # 将 PIL 转换为 numpy
    image_np = np.array(pil_image)
    H, W = image_np.shape[:2]

    num_tokens = similarity_maps.size(0)
    assert num_tokens == len(query_tokens), (
        f"The number of query tokens in similarity_maps ({num_tokens}) "
        f"doesn't match the length of query_tokens list ({len(query_tokens)})."
    )

    fig, axs = plt.subplots(1, num_tokens, figsize=(5 * num_tokens, 5))
    if num_tokens == 1:
        # 如果只有一个标记，axs 将不是可迭代的
        axs = [axs]

    for idx in range(num_tokens):
        # 每个查询标记的相似性映射：形状 = (n_patches_x, n_patches_y)
        single_map = similarity_maps[idx]  # （n_patches_x, n_patches_y）

        # 直接使用原始补丁分辨率，不进行插值
        upsampled = single_map.squeeze()  # (n_patches_x, n_patches_y)

        # 如果你的映射是bfloat16，使用.to(torch.float32)修复
        heatmap = upsampled.squeeze().to(torch.float32).cpu().numpy()  # （高, 宽）

        # 可选地归一化热图（如果需要请取消注释）
        # heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-8)

        # 绘制
        axs[idx].imshow(image_np, cmap=None if image_np.ndim == 3 else 'gray')
        axs[idx].imshow(heatmap, cmap='jet', alpha=alpha)
        axs[idx].set_title(f"Query: {query_tokens[idx]}")
        axs[idx].axis('off')

    plt.tight_layout()
    plt.show()