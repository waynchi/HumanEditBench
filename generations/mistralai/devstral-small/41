from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Crear una sesión Spark
spark = SparkSession.builder.appName("EscrituraParquetADL2").getOrCreate()

# Analizar los datos provenientes de la API (reemplazar con tu lógica de análisis)
# Aquí puedes agregar código para inspeccionar el esquema, muestras de datos, etc.
# Ejemplo básico de cómo podrías analizar los datos:

# 1. Mostrar el esquema del DataFrame
df.printSchema()

# 2. Mostrar las primeras filas
df.show(5)

# 3. Mostrar estadísticas descriptivas
df.describe().show()

# 4. Verificar valores nulos
from pyspark.sql.functions import col, count
df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()

# Configurar la conexión a ADL2 usando la identidad de Microsoft ID
# No es necesario proporcionar credenciales explícitamente en un notebook de Synapse
# Spark utilizará la identidad administrada del notebook para autenticarse.

# Especificar la ruta al contenedor y la carpeta en ADL2
container_name = "<your_container_name>"  # Reemplazar con el nombre de tu contenedor
folder_path = "<your_folder_path>"  # Reemplazar con la ruta a la carpeta dentro del contenedor
adl2_path = f"abfss://{container_name}@{<your_storage_account_name>}.dfs.core.windows.net/{folder_path}"

# Escribir el DataFrame en formato parquet en ADL2
df.write.parquet(adl2_path, mode="overwrite")

# Opcional: leer el archivo parquet para verificar
df_leido = spark.read.parquet(adl2_path)
df_leido.show()

# Detener la sesión Spark
spark.stop()